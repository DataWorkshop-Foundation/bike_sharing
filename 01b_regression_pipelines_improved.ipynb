{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Main info*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Project description:*** https://www.kaggle.com/competitions/bike-sharing-demand/overview/description\n",
    "\n",
    "***Project goal:*** combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n",
    "\n",
    "***Suggested evaluation metric:*** Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "***Other used evalutaion metrics*** Mean Absolute Error(MAE), Mean Squared Error(MSE), Root Mean Squared Error(RMSE), R Squared (R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***0. Project preparation*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main upgrades\n",
    "!pip install --upgrade neptune-client\n",
    "!pip install --upgrade neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "import catboost as ctb\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "import eli5\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# minor settings\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "set_config(display='diagram')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# global variables\n",
    "PAD = 20\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.5\n",
      "numpy...............1.20.0\n",
      "pandas..............1.2.4\n",
      "sklearn.............0.24.2\n",
      "eli5................0.11.0\n",
      "neptune.............0.15.2\n"
     ]
    }
   ],
   "source": [
    "# version check\n",
    "def show_version(module_object: object, n: int = PAD) -> str:\n",
    "    '''\n",
    "    Check version of different libraries\n",
    "    '''\n",
    "    module_name = getattr(module_object, '__name__')\n",
    "    module_ver = getattr(module_object, '__version__')\n",
    "    dots = '.' * (n - len(module_name))\n",
    "    \n",
    "    print (f'{module_name}{dots}{module_ver}')\n",
    "\n",
    "\n",
    "!python --version\n",
    "module_list = [np, pd, sklearn, eli5, neptune]\n",
    "for module in module_list:\n",
    "    show_version(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run neptune server\n",
    "def init_neptune():\n",
    "    '''\n",
    "    Initialize neptune project\n",
    "    '''\n",
    "    with open('neptune_credentials') as f:\n",
    "        neptune_credentials = json.load(f)\n",
    "\n",
    "    run = neptune.init(\n",
    "        api_token = neptune_credentials['API_TOKEN'],\n",
    "        project_qualified_name = neptune_credentials['PROJECT']\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***1. Load data*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('inputs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {1: ['Clear', 'Few clouds', 'Partly cloudy'],\n",
    "2: ['Mist + Cloudy', 'Mist + Broken clouds', 'Mist + Few clouds', 'Mist'],\n",
    "3: ['Light Snow', 'Light Rain + Thunderstorm + Scattered clouds', 'Light Rain + Scattered clouds'],\n",
    "4: ['Heavy Rain + Ice Pallets + Thunderstorm + Mist', 'Snow + Fog']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2. Custom data classes*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    '''\n",
    "    Change the initial dataset into a new one based\n",
    "    on passed function\n",
    "    '''\n",
    "    # copy parameter introduced to prevent SettingwithCopyWarning\n",
    "    # https://www.dataquest.io/blog/settingwithcopywarning/\n",
    "    def __init__(self, func, copy = True, **kwargs):\n",
    "        self.func = func\n",
    "        self.copy = copy\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        input_df_ = input_df if not self.copy else input_df.copy()\n",
    "        return self.func(input_df_,)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector():\n",
    "    '''\n",
    "    Return a dataframe with predefined columns only\n",
    "    '''\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDroper():\n",
    "    '''\n",
    "    Return a dataframe without selected columns\n",
    "    '''\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3. Custom feature functions*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.1. Add or change features*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cols_from_datetime(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Convert a column to datetime format and create new columns: \n",
    "    'year', 'month', 'day', 'hour'\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame \n",
    "    \n",
    "    Returns:\n",
    "        dataset: transformed pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    # convert string to datetime type\n",
    "    dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "    # make new columns from datetime column\n",
    "    dataset['year'] = dataset['datetime'].dt.year\n",
    "    dataset['month'] = dataset['datetime'].dt.month\n",
    "    dataset['day'] = dataset['datetime'].dt.day\n",
    "    dataset['hour'] = dataset['datetime'].dt.hour\n",
    "    dataset['dayofweek'] = dataset['datetime'].dt.dayofweek\n",
    "    dataset['weekend'] = dataset['dayofweek'].map(lambda x: int(x in [6,7]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons_change(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Set proper season duration and change their representation according\n",
    "    to dataset legend\n",
    "\n",
    "    Argument:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    changes = [\n",
    "    ('2011-01-01', '2011-03-19', 4),\n",
    "    ('2011-03-20', '2011-06-20', 1),\n",
    "    ('2011-06-21', '2011-09-22', 2),\n",
    "    ('2011-09-23', '2011-12-20', 3),\n",
    "    ('2011-12-21', '2012-03-19', 4),\n",
    "    ('2012-03-20', '2012-06-19', 1),\n",
    "    ('2012-06-20', '2012-09-21', 2),\n",
    "    ('2012-09-22', '2012-12-20', 3),\n",
    "    ('2012-12-21', '2012-12-31', 4),\n",
    "     ]\n",
    "\n",
    "    for (start_date, end_date, new_season) in changes:\n",
    "        dataset.loc[between_dates(dataset, start_date, '00', end_date, '23').index,'season'] = new_season\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agg_features(dataset: pd.DataFrame, agg_name = np.median) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate a monthly agg_function (like mean, median...) and add it to the dataframe\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: DataFrame with an additional column which contains the agg_function\n",
    "        of bike shares in a month\n",
    "    '''\n",
    "    agg_dataset = dataset[['month', 'year', 'count']].groupby(['month', 'year']).agg(agg_name)\n",
    "    agg_dataset = agg_dataset.reset_index()\n",
    "    agg_dataset = agg_dataset.rename(columns = {'count':str(agg_name.__name__)})\n",
    "    return pd.merge(dataset, agg_dataset, on=['month', 'year'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize_feats(dataset: pd.DataFrame, feats: list = []) -> pd.DataFrame:\n",
    "    '''\n",
    "    Normalize selected features. Equivalent of MinMaxScaler\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    for feature_name in feats:\n",
    "        if feature_name in dataset.columns.tolist():\n",
    "            max_value = dataset[feature_name].max()\n",
    "            min_value = dataset[feature_name].min()\n",
    "            dataset[feature_name] = (dataset[feature_name] - min_value) / (max_value - min_value)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_weather(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Change integers in 'weather' column (1, 2, 3, ...) into a list of weather\n",
    "    phenomena\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        initial dataframe with one extra column (weather_phenomena)\n",
    "    '''\n",
    "    \n",
    "    # split weather phenomena into individual elements\n",
    "    def weather_to_indicidual(x):\n",
    "        x = ','.join([elem for elem in x])\n",
    "        x = re.split(r'[,\\+]', x)\n",
    "\n",
    "        return list(set([elem.strip() for elem in x]))\n",
    "\n",
    "    # change integers in 'weather' column  into list of individual weather phenomena\n",
    "    dataset['weather_phenomena'] = dataset['weather'].map(weather_dict).apply(lambda x: weather_to_indicidual(x))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def deconstruct_weather_tokens(dataset: pd.DataFrame, token_col_name: str = 'weather_phenomena') -> pd.DataFrame:\n",
    "    '''\n",
    "    One-hot-encode individual weather phenomena and add them to initial dataframe\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        initial dataframe with an extra columns containing 0 or 1; each column\n",
    "        represents an individual weather phenomena\n",
    "    '''\n",
    "\n",
    "    # make a list of all individual meteorological phenomena \n",
    "    all_phenomena = list(dataset[token_col_name].explode().unique())\n",
    "\n",
    "    def return_cols_with_individual_phenomena(phenomena: pd.Series) -> pd.DataFrame:\n",
    "        '''\n",
    "        Build a dataframe (X) with all_phenomena as columns and put 0 or 1 in \n",
    "        rows representing the presence or absence of a particular phenomena\n",
    "        '''\n",
    "\n",
    "        def phenomenon_in_phenomena(phenomena: pd.Series) -> list:\n",
    "            return [int(phenomenon in phenomena) for phenomenon in all_phenomena]\n",
    "    \n",
    "        X = phenomena.map(phenomenon_in_phenomena).apply(pd.Series)\n",
    "        X.columns = all_phenomena\n",
    "        return X \n",
    "    \n",
    "    X = return_cols_with_individual_phenomena(dataset['weather_phenomena'])\n",
    "\n",
    "    # concatenate columns to original dataset\n",
    "    return pd.concat([dataset, X], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weather_embeddings(dataset: pd.DataFrame) -> np.array:\n",
    "    '''\n",
    "    Return unique vector representations of the weather column\n",
    "    \n",
    "    Arguments: \n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        array of 4 lists (4 types of weather), each list containing 384 elements\n",
    "    '''\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # randomly chosen pretrained model\n",
    "    model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "    # column of unique meteo phenomena in a 'str' form\n",
    "    uni_meteo_weather = dataset['weather_phenomena'].map(lambda x: ' '.join(x)).unique() \n",
    "\n",
    "    # vector representatnions of 'uni_meteo_weather'\n",
    "    embeddings = model.encode(uni_meteo_weather, convert_to_tensor = False)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_weather_nlp(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return vector representations of the weather column\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset with 384 more columns containing vector representation of sentences\n",
    "        (meteorological phenomena contatenated into a string are treated as sentences)\n",
    "    '''\n",
    "\n",
    "    # unique integers (1, 2, 3, 4) repesenting the weather\n",
    "    uni_num_weather = dataset['weather'].unique()\n",
    "    \n",
    "    # unique embeddings of weather phenomena\n",
    "    embeddings = make_weather_embeddings(dataset)\n",
    "\n",
    "    # dictionary for pandas mapping\n",
    "    embeddings_dict = dict(zip(uni_num_weather, embeddings))\n",
    "\n",
    "    # one additional column containing a 384-element list in each cell\n",
    "    dataset['weather_embeddings'] = dataset['weather'].map(embeddings_dict)\n",
    "\n",
    "    # split 'weather_embeddings' into 384 individual columns and concat them to the original dataset\n",
    "    dataset = pd.concat([dataset, pd.DataFrame(dataset['weather_embeddings'].tolist())], axis = 1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cos_sim_weather(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate cosinus similarity between 4 weather types and concat\n",
    "    4 additional columns to the original dataset containing calculated\n",
    "    similarity\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        original dataset with 4 extra columns representing cosine similarity \n",
    "        between weather types\n",
    "    '''\n",
    "\n",
    "    # construct 4x4 array of cosine similarity\n",
    "    cos_sims_array = peek_weather_cos_sim(dataset, return_dataframe= False)\n",
    "\n",
    "    # dictionary with cos_sims for pandas mapping \n",
    "    # {1: [1.000000 0.897642 0.728108 0.566511], 2: [...], 3: [...], 4: [...]}\n",
    "    cos_sims_dict = dict(zip(list(range(1,5)), cos_sims_array))\n",
    "\n",
    "    # map dictionary to pandas DataFrame as a new column\n",
    "    dataset['cos_sims'] = dataset['weather'].map(cos_sims_dict)\n",
    "\n",
    "    # deconstruct mapped column to 4 individual columns\n",
    "    cos_sims_new_columns = pd.DataFrame(dataset['cos_sims'].tolist(), columns = [f'cos_sim_weather_{n}' for n in list(range(1, 5))])\n",
    "\n",
    "    # delete not needed column\n",
    "    del dataset['cos_sims']\n",
    "\n",
    "    return pd.concat([dataset, cos_sims_new_columns], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(dataset: pd.DataFrame, feats: list, transformer: sklearn.preprocessing._data) -> pd.DataFrame:\n",
    "    '''\n",
    "    Use a transformer to transfor selected columns\n",
    "    Examples of transformers: OrdinalEncoder, MinMaxSxaler, Normalizer, StandardScaler\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        dataframe with transformed columns\n",
    "    '''\n",
    "    dataset[feats] = transformer.fit_transform(dataset[feats])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(dataset: pd.DataFrame, pipeline: Pipeline) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset after pipeline transformations\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "\n",
    "    Returns: \n",
    "        dataset after pipeline transformation\n",
    "    '''\n",
    "    return pipeline.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.2. Select features*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between_dates(dataset: pd.DataFrame, start_date: str, start_time: str, end_date: str, end_time: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Show dataframe between two dates and timestamps\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame \n",
    "        start_date: date which the dataset must be trimmed from\n",
    "        start_time: hour of day from start_date\n",
    "        end_date: date which the dataset must be trimmed to\n",
    "        end_time: hour of day from end_date\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame between (start_date, start_time) and (end_date, end_time)\n",
    "    '''\n",
    "\n",
    "    start_dt = f'{start_date} {start_time}:00:00'\n",
    "    end_dt = f'{end_date} {end_time}:00:00'\n",
    "    mask = (dataset['datetime'] >= start_dt) & (dataset['datetime'] <= end_dt)\n",
    "    return dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_obvious(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset withouth some obviously unneeded columns\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    black_list = ['Unnamed: 0', 'datetime', 'casual', 'registered']\n",
    "    \n",
    "    feats = [feat for feat in dataset.columns.tolist() if feat not in black_list]\n",
    "\n",
    "    return dataset[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dtypes(dataset: pd.DataFrame, dtypes = np.number) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset with a specific datatype\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    return dataset.select_dtypes(include=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y(dataset: pd.DataFrame, test_size: int = 0.3):\n",
    "    '''\n",
    "    Split dataframe info train and valid set\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "        test_size: test size split\n",
    "    \n",
    "    Returns:\n",
    "        two DataFrames and two DataSeries containing independent and target variables\n",
    "    '''\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(dataset.drop('count', axis = 1), dataset['count'], test_size = test_size, random_state = RANDOM_STATE)\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.3. Additional info features*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_weather_cos_sim(dataset: pd.DataFrame, return_dataframe = True):\n",
    "    '''\n",
    "    Show cosine similarity between different types of weather for a transformed dataset\n",
    "    \n",
    "    Arguments: \n",
    "        dataset: pandas DataFrame (transformed dataset containing 'weather_phenomena' column)\n",
    "    \n",
    "    Returns:\n",
    "       4 x 4 DataFrame with cosine similarity of weather types \n",
    "       or\n",
    "       dataframe with cosine similarity of weather types\n",
    "        \n",
    "    '''\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "    # unique embeddings of weather phenomena    \n",
    "    embeddings = make_weather_embeddings(dataset)\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    cos_sims = cosine_similarity(embeddings)\n",
    "\n",
    "    # how many different weather types there is\n",
    "    no_weather_types = len(weather_dict)\n",
    "\n",
    "    if return_dataframe:\n",
    "        return  pd.DataFrame(cos_sims, columns = list(range(1,no_weather_types + 1)), index = list(range(1,no_weather_types + 1)))\n",
    "    else:\n",
    "        return cos_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***4. Evaluation metrics*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> np.float64:\n",
    "    '''\n",
    "    The Root Mean Squared Log Error (RMSLE) metric \n",
    "\n",
    "    Arguments: \n",
    "        y_true: the ground truth labels given in the dataset\n",
    "        y_pred: our predictions\n",
    "        \n",
    "    Returns: \n",
    "        The RMSLE score\n",
    "    '''\n",
    "\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5. Pipelines and models*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocess = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('min_max_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['humidity', 'casual', 'windspeed'], MinMaxScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season']))),\n",
    "    ('tokenize_weather',\n",
    "                        DataTransformer(tokenize_weather)),\n",
    "    ('deconstruct_tokens',\n",
    "                        DataTransformer(deconstruct_weather_tokens)),\n",
    "    ('nlp',\n",
    "                        DataTransformer(vectorize_weather_nlp)),\n",
    "    ('add_cos_sim',\n",
    "                        DataTransformer(add_cos_sim_weather)),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['weather_phenomena', 'weather_embeddings', 'humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)\n",
    "\n",
    "no_nlp_pipeline = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('min_max_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['humidity', 'casual', 'windspeed'], MinMaxScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season']))),\n",
    "    ('tokenize_weather',\n",
    "                        DataTransformer(tokenize_weather)),\n",
    "    ('deconstruct_tokens',\n",
    "                        DataTransformer(deconstruct_weather_tokens)),\n",
    "    ('add_cos_sim',\n",
    "                        DataTransformer(add_cos_sim_weather)),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['weather_phenomena', 'humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)\n",
    "\n",
    "no_nlp_pipeline_no_cos_sim = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('min_max_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['humidity', 'casual', 'windspeed'], MinMaxScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season']))),\n",
    "    ('tokenize_weather',\n",
    "                        DataTransformer(tokenize_weather)),\n",
    "    ('deconstruct_tokens',\n",
    "                        DataTransformer(deconstruct_weather_tokens)),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['weather_phenomena', 'humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    ('main_preprocess', main_preprocess, 'Default preprocessing pipeline'),\n",
    "    ('no_nlp', no_nlp_pipeline, 'No nlp'),\n",
    "    ('no_nlp_no_cos_sim', no_nlp_pipeline_no_cos_sim, 'No nlp and no cos sim'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': RANDOM_STATE, 'verbosity':0, 'use_label_encoder': False }\n",
    "cparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': RANDOM_STATE , 'silent': True}\n",
    "lparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': RANDOM_STATE, 'verbosity':-1, 'silent': True}\n",
    "rfparams = {'random_state': RANDOM_STATE}\n",
    "dparams = {'strategy': 'median'}\n",
    "\n",
    "models = [\n",
    "    ('XGBoostRegressor', xgb.XGBRegressor(**xparams)),\n",
    "    ('CatBoostRregressor',  ctb.CatBoostRegressor(**cparams)),\n",
    "    ('LGBMRegressor', lgbm.LGBMRegressor(**lparams)),\n",
    "    ('RandomForest', RandomForestRegressor()),\n",
    "    ('DummyRegressor', DummyRegressor(**dparams)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***8. Main run*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(pipelines, models = models, experiments_common_name = 'test-experiments',  use_neptune = True, pickle_models = False, dump_pickled_models_to_neptune = False):\n",
    "    \n",
    "    # empty dataframe for locally keeping track of results\n",
    "    models_df = pd.DataFrame()\n",
    "\n",
    "    # total number of all experiments\n",
    "    no_experiments = len(list(product(pipelines, models)))\n",
    "\n",
    "    print(f'Running {no_experiments} experiments')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # initialize neptune if needed\n",
    "    if use_neptune:\n",
    "        run = init_neptune()\n",
    "    \n",
    "\n",
    "    # median of target values \n",
    "    df_median = np.median(df_train['count']) \n",
    "\n",
    "    n = 1\n",
    "    for p, pipeline_obj in enumerate(pipelines, 1):\n",
    "        pipeline_name = pipeline_obj[0] # name of pipeline\n",
    "        pipeline = pipeline_obj[1] # pipeline instance\n",
    "        pipeline_comment = pipeline_obj[2] # pipeline description\n",
    "\n",
    "        print(f'Transforming dataset using |{pipeline_name}|... (transformation {p}/{len(pipelines)})')\n",
    "        print(f'{\"=\"*60}')\n",
    "        df = transform_dataset(df_train, pipeline) # transform dataset using a pipline\n",
    "        X_train, X_valid, y_train, y_valid = make_X_y(df) # split dataset into train and valid\n",
    "        \n",
    "    \n",
    "        for m, model_object in enumerate(models, 1):\n",
    "\n",
    "            model_name = model_object[0] # custom name of a  model (like 'XGBoostRegressor')\n",
    "            model = model_object[1] # model instance\n",
    "\n",
    "            print('\\n')\n",
    "            print(f'Fitting... (model {m}/{len(models)})')\n",
    "            print(model_name)\n",
    "        \n",
    "            if use_neptune:\n",
    "                neptune.create_experiment(f'{experiments_common_name}-{n}') # name of experiment\n",
    "            \n",
    "            n += 1\n",
    "            start_time = time.time() # time fitting\n",
    "            model.fit(X_train, y_train) # fit the model\n",
    "            end_time = time.time()\n",
    "\n",
    "            # file pickling\n",
    "            if pickle_models: \n",
    "                \n",
    "                # file names\n",
    "                data_file_name = f'{pipeline_name}.csv'\n",
    "                model_file_name = f'{model_name}-{pipeline_name}.model'\n",
    "                \n",
    "                df.to_csv(data_file_name) # save dataframe locally\n",
    "                \n",
    "                # save model locally\n",
    "                with open(model_file_name, 'wb') as f:\n",
    "                    pickle.dump(model, f) # pickle a model\n",
    "                \n",
    "                if use_neptune and dump_pickled_models_to_neptune:\n",
    "                    # if specified dump pickled files to neptune\n",
    "                    neptune.log_artifact(model_file_name)\n",
    "                    neptune.log_artifact(data_file_name)\n",
    "\n",
    "            y_pred = model.predict(X_valid) # predicted values\n",
    "            y_pred = [df_median if y<0 else y for y in y_pred] # no negative values\n",
    "\n",
    "            # metrics (scores)\n",
    "            score_mae = mean_absolute_error(y_valid, y_pred)\n",
    "            score_mse = mean_squared_error(y_valid, y_pred)\n",
    "            score_rmse = np.sqrt(score_mse) \n",
    "            score_rmsle = rmsle(y_valid, y_pred)\n",
    "            score_r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "            model_params = str(model.get_params()) # model parameters\n",
    "            \n",
    "            # dictionary of all variables that are supposed to be logged\n",
    "            param_dict = {\n",
    "                'pipeline_name': pipeline_name,\n",
    "                'pipeline_steps': str(list(pipeline.named_steps.keys())),\n",
    "                'pipeline_comment': pipeline_comment,\n",
    "                'feats': str(X_train.columns.tolist()),\n",
    "                'model': model.__class__.__name__,\n",
    "                'model_params': model_params,\n",
    "                'score_mae': score_mae,\n",
    "                'score_mse': score_mse,\n",
    "                'score_rmse': score_rmse,\n",
    "                'score_rmsle': score_rmsle,\n",
    "                'score_r2': score_r2,\n",
    "                'time_elapsed': end_time - start_time\n",
    "            }\n",
    "\n",
    "            # log into neptune if needed\n",
    "            if use_neptune:\n",
    "\n",
    "                # make a list ['score_mae', 'score_mse', 'score_rmse', 'score_rmsle', 'score_r2', 'time_elapsed']\n",
    "                score_metrics = [elem for elem in list(param_dict.keys()) if elem.startswith('score_')] + ['time_elapsed']\n",
    "                \n",
    "                # log values depending on their type (str or float)\n",
    "                for key, value in param_dict.items():\n",
    "                    if key not in score_metrics:\n",
    "                        neptune.log_text(key, value)\n",
    "                    else:\n",
    "                        neptune.log_metric(key, value)\n",
    "                \n",
    "            # add row into summary dataframe for local results\n",
    "            models_df = models_df.append(pd.DataFrame(param_dict, index = [0]))\n",
    "    \n",
    "    models_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # end neptune instance\n",
    "    if use_neptune:\n",
    "        neptune.stop()\n",
    "    \n",
    "    return models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 15 experiments\n",
      "============================================================\n",
      "Transforming dataset using |main_preprocess|... (transformation 1/3)\n",
      "============================================================\n",
      "[Pipeline] .. (step 1 of 12) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 12) Processing change_seasons, total=   0.0s\n",
      "[Pipeline] ........ (step 3 of 12) Processing add_means, total=   0.0s\n",
      "[Pipeline] ...... (step 4 of 12) Processing add_medians, total=   0.0s\n",
      "[Pipeline] .... (step 5 of 12) Processing min_max_scale, total=   0.0s\n",
      "[Pipeline] ..... (step 6 of 12) Processing drop_obvious, total=   0.0s\n",
      "[Pipeline] .......... (step 7 of 12) Processing dummies, total=   0.0s\n",
      "[Pipeline] . (step 8 of 12) Processing tokenize_weather, total=   0.1s\n",
      "[Pipeline]  (step 9 of 12) Processing deconstruct_tokens, total=   2.2s\n",
      "[Pipeline] ............. (step 10 of 12) Processing nlp, total=  13.7s\n",
      "[Pipeline] ..... (step 11 of 12) Processing add_cos_sim, total=  11.1s\n",
      "[Pipeline] .... (step 12 of 12) Processing drop_columns, total=   0.1s\n",
      "\n",
      "\n",
      "Fitting... (model 1/5)\n",
      "XGBoostRegressor\n",
      "\n",
      "\n",
      "Fitting... (model 2/5)\n",
      "CatBoostRregressor\n",
      "\n",
      "\n",
      "Fitting... (model 3/5)\n",
      "LGBMRegressor\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "\n",
      "\n",
      "Fitting... (model 4/5)\n",
      "RandomForest\n",
      "\n",
      "\n",
      "Fitting... (model 5/5)\n",
      "DummyRegressor\n",
      "Transforming dataset using |no_nlp|... (transformation 2/3)\n",
      "============================================================\n",
      "[Pipeline] .. (step 1 of 11) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 11) Processing change_seasons, total=   0.0s\n",
      "[Pipeline] ........ (step 3 of 11) Processing add_means, total=   0.0s\n",
      "[Pipeline] ...... (step 4 of 11) Processing add_medians, total=   0.0s\n",
      "[Pipeline] .... (step 5 of 11) Processing min_max_scale, total=   0.0s\n",
      "[Pipeline] ..... (step 6 of 11) Processing drop_obvious, total=   0.0s\n",
      "[Pipeline] .......... (step 7 of 11) Processing dummies, total=   0.0s\n",
      "[Pipeline] . (step 8 of 11) Processing tokenize_weather, total=   0.1s\n",
      "[Pipeline]  (step 9 of 11) Processing deconstruct_tokens, total=   4.6s\n",
      "[Pipeline] ..... (step 10 of 11) Processing add_cos_sim, total=  10.9s\n",
      "[Pipeline] .... (step 11 of 11) Processing drop_columns, total=   0.0s\n",
      "\n",
      "\n",
      "Fitting... (model 1/5)\n",
      "XGBoostRegressor\n",
      "\n",
      "\n",
      "Fitting... (model 2/5)\n",
      "CatBoostRregressor\n",
      "\n",
      "\n",
      "Fitting... (model 3/5)\n",
      "LGBMRegressor\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "\n",
      "\n",
      "Fitting... (model 4/5)\n",
      "RandomForest\n",
      "\n",
      "\n",
      "Fitting... (model 5/5)\n",
      "DummyRegressor\n",
      "Transforming dataset using |no_nlp_no_cos_sim|... (transformation 3/3)\n",
      "============================================================\n",
      "[Pipeline] .. (step 1 of 10) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 10) Processing change_seasons, total=   0.0s\n",
      "[Pipeline] ........ (step 3 of 10) Processing add_means, total=   0.0s\n",
      "[Pipeline] ...... (step 4 of 10) Processing add_medians, total=   0.0s\n",
      "[Pipeline] .... (step 5 of 10) Processing min_max_scale, total=   0.0s\n",
      "[Pipeline] ..... (step 6 of 10) Processing drop_obvious, total=   0.0s\n",
      "[Pipeline] .......... (step 7 of 10) Processing dummies, total=   0.0s\n",
      "[Pipeline] . (step 8 of 10) Processing tokenize_weather, total=   0.0s\n",
      "[Pipeline]  (step 9 of 10) Processing deconstruct_tokens, total=   3.0s\n",
      "[Pipeline] .... (step 10 of 10) Processing drop_columns, total=   0.0s\n",
      "\n",
      "\n",
      "Fitting... (model 1/5)\n",
      "XGBoostRegressor\n",
      "\n",
      "\n",
      "Fitting... (model 2/5)\n",
      "CatBoostRregressor\n",
      "\n",
      "\n",
      "Fitting... (model 3/5)\n",
      "LGBMRegressor\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "\n",
      "\n",
      "Fitting... (model 4/5)\n",
      "RandomForest\n",
      "\n",
      "\n",
      "Fitting... (model 5/5)\n",
      "DummyRegressor\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>pipeline_steps</th>\n",
       "      <th>pipeline_comment</th>\n",
       "      <th>feats</th>\n",
       "      <th>model</th>\n",
       "      <th>model_params</th>\n",
       "      <th>score_mae</th>\n",
       "      <th>score_mse</th>\n",
       "      <th>score_rmse</th>\n",
       "      <th>score_rmsle</th>\n",
       "      <th>score_r2</th>\n",
       "      <th>time_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocessing pipeline</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>24.865155</td>\n",
       "      <td>1857.849823</td>\n",
       "      <td>43.102782</td>\n",
       "      <td>0.396177</td>\n",
       "      <td>0.942791</td>\n",
       "      <td>4.290999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocessing pipeline</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>27.201225</td>\n",
       "      <td>2016.683588</td>\n",
       "      <td>44.907500</td>\n",
       "      <td>0.670137</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>3.722847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocessing pipeline</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.922237</td>\n",
       "      <td>2102.292492</td>\n",
       "      <td>45.850763</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>1.440002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocessing pipeline</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>25.945756</td>\n",
       "      <td>1912.771354</td>\n",
       "      <td>43.735242</td>\n",
       "      <td>0.333867</td>\n",
       "      <td>0.941100</td>\n",
       "      <td>44.225128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocessing pipeline</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.212549</td>\n",
       "      <td>1873.620403</td>\n",
       "      <td>43.285337</td>\n",
       "      <td>0.408045</td>\n",
       "      <td>0.942306</td>\n",
       "      <td>0.992067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>26.378016</td>\n",
       "      <td>1887.264891</td>\n",
       "      <td>43.442662</td>\n",
       "      <td>0.615291</td>\n",
       "      <td>0.941885</td>\n",
       "      <td>1.576317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.922237</td>\n",
       "      <td>2102.292492</td>\n",
       "      <td>45.850763</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.163003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>25.744608</td>\n",
       "      <td>1880.432081</td>\n",
       "      <td>43.363949</td>\n",
       "      <td>0.329776</td>\n",
       "      <td>0.942096</td>\n",
       "      <td>3.950077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.000920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp and no cos sim</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.212549</td>\n",
       "      <td>1873.620403</td>\n",
       "      <td>43.285337</td>\n",
       "      <td>0.408045</td>\n",
       "      <td>0.942306</td>\n",
       "      <td>1.168135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp and no cos sim</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>26.582502</td>\n",
       "      <td>1944.410204</td>\n",
       "      <td>44.095467</td>\n",
       "      <td>0.619069</td>\n",
       "      <td>0.940126</td>\n",
       "      <td>1.480120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp and no cos sim</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.922237</td>\n",
       "      <td>2102.292492</td>\n",
       "      <td>45.850763</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.126963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp and no cos sim</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>26.072756</td>\n",
       "      <td>1930.783796</td>\n",
       "      <td>43.940685</td>\n",
       "      <td>0.333037</td>\n",
       "      <td>0.940545</td>\n",
       "      <td>3.741086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp and no cos sim</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pipeline_name                                     pipeline_steps  \\\n",
       "0     main_preprocess  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "1     main_preprocess  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "2     main_preprocess  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "3     main_preprocess  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "4     main_preprocess  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "5              no_nlp  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "6              no_nlp  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "7              no_nlp  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "8              no_nlp  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "9              no_nlp  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "10  no_nlp_no_cos_sim  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "11  no_nlp_no_cos_sim  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "12  no_nlp_no_cos_sim  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "13  no_nlp_no_cos_sim  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "14  no_nlp_no_cos_sim  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "\n",
       "                  pipeline_comment  \\\n",
       "0   Default preprocessing pipeline   \n",
       "1   Default preprocessing pipeline   \n",
       "2   Default preprocessing pipeline   \n",
       "3   Default preprocessing pipeline   \n",
       "4   Default preprocessing pipeline   \n",
       "5                           No nlp   \n",
       "6                           No nlp   \n",
       "7                           No nlp   \n",
       "8                           No nlp   \n",
       "9                           No nlp   \n",
       "10           No nlp and no cos sim   \n",
       "11           No nlp and no cos sim   \n",
       "12           No nlp and no cos sim   \n",
       "13           No nlp and no cos sim   \n",
       "14           No nlp and no cos sim   \n",
       "\n",
       "                                                feats                  model  \\\n",
       "0   ['holiday', 'workingday', 'weather', 'atemp', ...           XGBRegressor   \n",
       "1   ['holiday', 'workingday', 'weather', 'atemp', ...      CatBoostRegressor   \n",
       "2   ['holiday', 'workingday', 'weather', 'atemp', ...          LGBMRegressor   \n",
       "3   ['holiday', 'workingday', 'weather', 'atemp', ...  RandomForestRegressor   \n",
       "4   ['holiday', 'workingday', 'weather', 'atemp', ...         DummyRegressor   \n",
       "5   ['holiday', 'workingday', 'weather', 'atemp', ...           XGBRegressor   \n",
       "6   ['holiday', 'workingday', 'weather', 'atemp', ...      CatBoostRegressor   \n",
       "7   ['holiday', 'workingday', 'weather', 'atemp', ...          LGBMRegressor   \n",
       "8   ['holiday', 'workingday', 'weather', 'atemp', ...  RandomForestRegressor   \n",
       "9   ['holiday', 'workingday', 'weather', 'atemp', ...         DummyRegressor   \n",
       "10  ['holiday', 'workingday', 'weather', 'atemp', ...           XGBRegressor   \n",
       "11  ['holiday', 'workingday', 'weather', 'atemp', ...      CatBoostRegressor   \n",
       "12  ['holiday', 'workingday', 'weather', 'atemp', ...          LGBMRegressor   \n",
       "13  ['holiday', 'workingday', 'weather', 'atemp', ...  RandomForestRegressor   \n",
       "14  ['holiday', 'workingday', 'weather', 'atemp', ...         DummyRegressor   \n",
       "\n",
       "                                         model_params   score_mae  \\\n",
       "0   {'objective': 'reg:squarederror', 'base_score'...   24.865155   \n",
       "1   {'loss_function': 'RMSE', 'silent': True, 'max...   27.201225   \n",
       "2   {'boosting_type': 'gbdt', 'class_weight': None...   27.922237   \n",
       "3   {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   25.945756   \n",
       "4   {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "5   {'objective': 'reg:squarederror', 'base_score'...   25.212549   \n",
       "6   {'loss_function': 'RMSE', 'silent': True, 'max...   26.378016   \n",
       "7   {'boosting_type': 'gbdt', 'class_weight': None...   27.922237   \n",
       "8   {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   25.744608   \n",
       "9   {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "10  {'objective': 'reg:squarederror', 'base_score'...   25.212549   \n",
       "11  {'loss_function': 'RMSE', 'silent': True, 'max...   26.582502   \n",
       "12  {'boosting_type': 'gbdt', 'class_weight': None...   27.922237   \n",
       "13  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   26.072756   \n",
       "14  {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "\n",
       "       score_mse  score_rmse  score_rmsle  score_r2  time_elapsed  \n",
       "0    1857.849823   43.102782     0.396177  0.942791      4.290999  \n",
       "1    2016.683588   44.907500     0.670137  0.937900      3.722847  \n",
       "2    2102.292492   45.850763     0.586519  0.935264      1.440002  \n",
       "3    1912.771354   43.735242     0.333867  0.941100     44.225128  \n",
       "4   35329.691978  187.961943     1.462466 -0.087909      0.000000  \n",
       "5    1873.620403   43.285337     0.408045  0.942306      0.992067  \n",
       "6    1887.264891   43.442662     0.615291  0.941885      1.576317  \n",
       "7    2102.292492   45.850763     0.586519  0.935264      0.163003  \n",
       "8    1880.432081   43.363949     0.329776  0.942096      3.950077  \n",
       "9   35329.691978  187.961943     1.462466 -0.087909      0.000920  \n",
       "10   1873.620403   43.285337     0.408045  0.942306      1.168135  \n",
       "11   1944.410204   44.095467     0.619069  0.940126      1.480120  \n",
       "12   2102.292492   45.850763     0.586519  0.935264      0.126963  \n",
       "13   1930.783796   43.940685     0.333037  0.940545      3.741086  \n",
       "14  35329.691978  187.961943     1.462466 -0.087909      0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiments(pipelines, models, use_neptune= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "423b8b96d4e1ec477b5c8920e05a523372dc7245eb6302a2ca5de114e24c04c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
