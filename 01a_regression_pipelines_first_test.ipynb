{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Project description*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Project description:*** https://www.kaggle.com/competitions/bike-sharing-demand/overview/description\n",
    "\n",
    "***Project goal:*** combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n",
    "\n",
    "***Suggested evaluation metric:*** Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "***Other used evalutaion metrics*** Mean Absolute Error(MAE), Mean Squared Error(MSE), Root Mean Squared Error(RMSE), R Squared (R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***0. Project preparation*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main upgrades\n",
    "!pip install --upgrade neptune-client\n",
    "!pip install --upgrade neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "import catboost as ctb\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "import eli5\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "from itertools import product\n",
    "\n",
    "# minor settings\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "set_config(display='diagram')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# global variables\n",
    "PAD = 20\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.5\n",
      "numpy...............1.20.0\n",
      "pandas..............1.2.4\n",
      "sklearn.............0.24.2\n",
      "eli5................0.11.0\n",
      "neptune.............0.15.2\n"
     ]
    }
   ],
   "source": [
    "# version check\n",
    "def show_version(module_object: object, n: int = PAD) -> str:\n",
    "    module_name = getattr(module_object, '__name__')\n",
    "    module_ver = getattr(module_object, '__version__')\n",
    "    dots = '.' * (n - len(module_name))\n",
    "    \n",
    "    print (f'{module_name}{dots}{module_ver}')\n",
    "\n",
    "\n",
    "!python --version\n",
    "module_list = [np, pd, sklearn, eli5, neptune]\n",
    "for module in module_list:\n",
    "    show_version(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(DataWorkshop-Foundation/bike-sharing)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neptune init\n",
    "with open('neptune_credentials.json') as f:\n",
    "   neptune_credentials = json.load(f)\n",
    "\n",
    "neptune.init(\n",
    "    api_token = neptune_credentials['API_TOKEN'],\n",
    "    project_qualified_name = neptune_credentials['PROJECT']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***1. Load data*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('inputs/train.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['count'], axis=1),\n",
    "                                                    df_train['count'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=RANDOM_STATE)\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_numerical = ['temp', 'atemp', 'humidity', 'windspeed']\n",
    "cols_categorical = ['season', 'holiday', 'workingday', 'weather', 'month', 'year', 'day', 'hour', 'dayofweek', 'weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>2012-12-19 19:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.58</td>\n",
       "      <td>19.695</td>\n",
       "      <td>50</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>7</td>\n",
       "      <td>329</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>2012-12-19 20:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.76</td>\n",
       "      <td>17.425</td>\n",
       "      <td>57</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>10</td>\n",
       "      <td>231</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>2012-12-19 21:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>15.910</td>\n",
       "      <td>61</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>2012-12-19 22:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>17.425</td>\n",
       "      <td>61</td>\n",
       "      <td>6.0032</td>\n",
       "      <td>12</td>\n",
       "      <td>117</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>2012-12-19 23:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.12</td>\n",
       "      <td>16.665</td>\n",
       "      <td>66</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime  season  holiday  ...  casual  registered  count\n",
       "0      2011-01-01 00:00:00       1        0  ...       3          13     16\n",
       "1      2011-01-01 01:00:00       1        0  ...       8          32     40\n",
       "2      2011-01-01 02:00:00       1        0  ...       5          27     32\n",
       "3      2011-01-01 03:00:00       1        0  ...       3          10     13\n",
       "4      2011-01-01 04:00:00       1        0  ...       0           1      1\n",
       "...                    ...     ...      ...  ...     ...         ...    ...\n",
       "10881  2012-12-19 19:00:00       4        0  ...       7         329    336\n",
       "10882  2012-12-19 20:00:00       4        0  ...      10         231    241\n",
       "10883  2012-12-19 21:00:00       4        0  ...       4         164    168\n",
       "10884  2012-12-19 22:00:00       4        0  ...      12         117    129\n",
       "10885  2012-12-19 23:00:00       4        0  ...       4          84     88\n",
       "\n",
       "[10886 rows x 12 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2. Custom data functions*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_datasets(dataset_1: pd.DataFrame, dataset_2: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Concatenate two datasets\n",
    "\n",
    "    Arguments:\n",
    "        dataset_1: first dataset to join\n",
    "        dataset_2: second dataset to join\n",
    "    \n",
    "    Returns:\n",
    "        Joined dataset\n",
    "    '''\n",
    "\n",
    "    merged_dataset = pd.concat([dataset_1, dataset_2]).reset_index(drop = True)\n",
    "    if 'datetime' in merged_dataset.columns:\n",
    "        merged_dataset.sort_values(by='datetime', inplace = True)\n",
    "    return merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform_data(dataset, pipeline_name):\n",
    "    dataset = pipeline_name.fit_transform(dataset)\n",
    "    X = dataset.drop(['count'], axis = 1).copy()\n",
    "    y = dataset['count']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_dataset(X, y):\n",
    "    return pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3. Custom data classes*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    '''\n",
    "    Change the initial dataset into a new one based\n",
    "    on passed function\n",
    "    '''\n",
    "    # copy parameter introduced to prevent SettingwithCopyWarning\n",
    "    # https://www.dataquest.io/blog/settingwithcopywarning/\n",
    "    def __init__(self, func, copy = True, **kwargs):\n",
    "        self.func = func\n",
    "        self.copy = copy\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        input_df_ = input_df if not self.copy else input_df.copy()\n",
    "        return self.func(input_df_,)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector():\n",
    "    '''\n",
    "    Return a dataframe with predefined columns only\n",
    "    '''\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDroper():\n",
    "    '''\n",
    "    Return a dataframe without selected columns\n",
    "    '''\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***4. Custom feature functions*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***4.1. Add or change features*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cols_from_datetime(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Convert a column to datetime format and create new columns: \n",
    "    'year', 'month', 'day', 'hour'\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame \n",
    "    \n",
    "    Returns:\n",
    "        dataset: transformed pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    # convert string to datetime type\n",
    "    dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "    # make new columns from datetime column\n",
    "    dataset['year'] = dataset['datetime'].dt.year\n",
    "    dataset['month'] = dataset['datetime'].dt.month\n",
    "    dataset['day'] = dataset['datetime'].dt.day\n",
    "    dataset['hour'] = dataset['datetime'].dt.hour\n",
    "    dataset['dayofweek'] = dataset['datetime'].dt.dayofweek\n",
    "    dataset['weekend'] = dataset['dayofweek'].map(lambda x: int(x in [6,7]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons_change(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Set proper season duration and change their representation according\n",
    "    to dataset legend\n",
    "\n",
    "    Argument:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    changes = [\n",
    "    ('2011-01-01', '2011-03-19', 4),\n",
    "    ('2011-03-20', '2011-06-20', 1),\n",
    "    ('2011-06-21', '2011-09-22', 2),\n",
    "    ('2011-09-23', '2011-12-20', 3),\n",
    "    ('2011-12-21', '2012-03-19', 4),\n",
    "    ('2012-03-20', '2012-06-19', 1),\n",
    "    ('2012-06-20', '2012-09-21', 2),\n",
    "    ('2012-09-22', '2012-12-20', 3),\n",
    "    ('2012-12-21', '2012-12-31', 4),\n",
    "     ]\n",
    "\n",
    "    for (start_date, end_date, new_season) in changes:\n",
    "        dataset.loc[between_dates(dataset, start_date, '00', end_date, '23').index,'season'] = new_season\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agg_features(dataset: pd.DataFrame, agg_name = np.median) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate a monthly agg_function (like mean, median...) and add it to the dataframe\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: DataFrame with an additional column which contains the agg_function\n",
    "        of bike shares in a month\n",
    "    '''\n",
    "    agg_dataset = dataset[['month', 'year', 'count']].groupby(['month', 'year']).agg(agg_name)\n",
    "    agg_dataset = agg_dataset.reset_index()\n",
    "    agg_dataset = agg_dataset.rename(columns = {'count':str(agg_name.__name__)})\n",
    "    return pd.merge(dataset, agg_dataset, on=['month', 'year'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_X(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Normalize the X variables. Equivalent of MinMaxScaler\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    feats = select_X_dtypes(dataset, dtypes = np.number).columns\n",
    "    result = dataset[feats].copy()\n",
    "    for feature_name in result.columns:\n",
    "        max_value = dataset[feature_name].max()\n",
    "        min_value = dataset[feature_name].min()\n",
    "        result[feature_name] = (dataset[feature_name] - min_value) / (max_value - min_value)\n",
    "    result['count'] = dataset['count']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_y(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Apply log + 1 function to target variable\n",
    "    '''\n",
    "    dataset['count'] = dataset['count'].apply(lambda x: np.log1p(x))\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***4.2. Select features*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between_dates(dataset: pd.DataFrame, start_date: str, start_time: str, end_date: str, end_time: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Show dataframe between two dates and timestamps\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame \n",
    "        start_date: date which the dataset must be trimmed from\n",
    "        start_time: hour of day from start_date\n",
    "        end_date: date which the dataset must be trimmed to\n",
    "        end_time: hour of day from end_date\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame between (start_date, start_time) and (end_date, end_time)\n",
    "    '''\n",
    "\n",
    "    start_dt = f'{start_date} {start_time}:00:00'\n",
    "    end_dt = f'{end_date} {end_time}:00:00'\n",
    "    mask = (dataset['datetime'] >= start_dt) & (dataset['datetime'] <= end_dt)\n",
    "    return dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats(dataset: pd.DataFrame, black_list = None, white_list = None) -> list:\n",
    "    '''\n",
    "    Return a list of features that appear in a given white_list and don't appear in a given \n",
    "    black_list\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        feats: list of features\n",
    "    '''\n",
    "    if white_list == None:\n",
    "        feats = dataset.columns.to_list()\n",
    "    else:\n",
    "        feats = white_list\n",
    "\n",
    "    if black_list is None:\n",
    "        black_list = ['Unnamed: 0', 'datetime', 'casual', 'registered']\n",
    "\n",
    "    feats = [feat for feat in feats if feat not in black_list]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_X_dtypes(dataset: pd.DataFrame, dtypes = np.number) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset with a specific data type but without the target variable \n",
    "\n",
    "    Argument:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    return dataset.drop('count', axis =1).select_dtypes(include=dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5. Evaluation metrics*** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> np.float64:\n",
    "    '''\n",
    "    The Root Mean Squared Log Error (RMSLE) metric \n",
    "\n",
    "    Arguments: \n",
    "        y_true: the ground truth labels given in the dataset\n",
    "        y_pred: our predictions\n",
    "        \n",
    "    Returns: \n",
    "        The RMSLE score\n",
    "    '''\n",
    "\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***6. Pipeline definitions*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***6.1. Standard num and cat pipeline definiton*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of transformers\n",
    "scalers = [StandardScaler(), MinMaxScaler(), Normalizer()]\n",
    "cat_transformers = [OrdinalEncoder(), OneHotEncoder()]\n",
    "\n",
    "\n",
    "transformer_numerical = Pipeline(steps = [\n",
    "    ('num_trans', StandardScaler())\n",
    "])\n",
    "\n",
    "transformer_categorical = Pipeline(steps = [\n",
    "    ('cat_trans', OneHotEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "num_cat_preprocess = ColumnTransformer(transformers = [\n",
    "    ('numerical', transformer_numerical, cols_numerical),\n",
    "    ('categorical', transformer_categorical, cols_categorical)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***6.2. Custom pipelines*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocess = Pipeline(steps = [\n",
    "    ('make_dt_columns', DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', DataTransformer(seasons_change)),\n",
    "    # ('add_means', DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    # ('add_medians', DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    # ('drop', ColumnDroper(['datetime']))\n",
    "], verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***7. Custom models*** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "xparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': 0, 'verbosity':0, 'use_label_encoder': False }\n",
    "cparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': 0 , 'silent': True}\n",
    "lparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': 0, 'verbosity':0, 'silent': True}\n",
    "\n",
    "models = [\n",
    "    ('XGBoostRegressor', xgb.XGBRegressor(**xparams)),\n",
    "    ('CatBoostRregressor',  ctb.CatBoostRegressor(**cparams)),\n",
    "    ('LGBMRegressor', lgbm.LGBMRegressor(**lparams)),\n",
    "    ('RandomForest', RandomForestRegressor())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***8. Main run*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-2\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/24 [00:04<01:35,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-3\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/24 [00:08<01:28,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-4\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 3/24 [00:12<01:25,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-5\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/24 [00:16<01:20,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-6\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5/24 [00:20<01:18,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-7\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 6/24 [00:26<01:24,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-8\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 7/24 [00:30<01:16,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-9\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8/24 [00:34<01:12,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-10\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 9/24 [00:38<01:05,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-11\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 10/24 [00:43<01:01,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-12\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [00:48<01:00,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-13\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12/24 [00:53<00:57,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-14\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13/24 [00:57<00:48,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-15\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14/24 [01:00<00:40,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-16\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 15/24 [01:03<00:34,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-17\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 16/24 [01:07<00:29,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-18\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [01:10<00:25,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-19\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 18/24 [01:13<00:21,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-20\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 19/24 [01:20<00:22,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-21\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 20/24 [02:05<01:06, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-22\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 21/24 [02:11<00:40, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-23\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [02:55<00:44, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-24\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 23/24 [03:02<00:17, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/DataWorkshop-Foundation/bike-sharing/e/BIKE-25\n",
      "[Pipeline] ... (step 1 of 2) Processing make_dt_columns, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 2) Processing change_seasons, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [03:46<00:00,  9.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# empty dataframe for locally keeping track of model results\n",
    "models_df = pd.DataFrame()\n",
    "\n",
    "# product of the lists (every possible combination of three lists)\n",
    "# it's needed because we will only make one for-loop\n",
    "# instead of three inner for-loops\n",
    "grid = list(product(models, scalers, cat_transformers))\n",
    "\n",
    "# pipeline template \n",
    "pipe = Pipeline(steps = [\n",
    "    ('main_preprocessing', main_preprocess),\n",
    "    ('num_cat_preprocessing', num_cat_preprocess),\n",
    "    ('regressor', None)\n",
    "])\n",
    "\n",
    "# some constants\n",
    "main_prep_step_name = pipe.steps[0][0] # name of the first pipeline in 'pipe'\n",
    "steps_in_main_prep = ', '.join([name[0] for name in pipe.steps[0][1].steps]) # list of steps in 'main_preprocessing'\n",
    "df_median = np.median(df_train['count']) # median of target value\n",
    "text_input = ['model', 'model_params', 'num_trans', 'cat_trans', 'prep_name', 'prep_steps'] # string values for logging\n",
    "\n",
    "\n",
    "for n, elem in enumerate(tqdm(grid)):\n",
    "    model = grid[n][0] # tuple (model_name, model_object)\n",
    "    num_tr = grid[n][1] # individual scaler, for example: StandardScaler()\n",
    "    cat_tr = grid[n][2] # individual categorical transformer, for example OneHotEncoder()\n",
    "\n",
    "    neptune.create_experiment(f'test-experiment-{n}') # name of experiment\n",
    "\n",
    "    pipe_params = {\n",
    "        'main_preprocessing': main_preprocess,\n",
    "        'num_cat_preprocessing__numerical__num_trans': num_tr,\n",
    "        'num_cat_preprocessing__categorical__cat_trans': cat_tr,\n",
    "        'regressor': model[1]\n",
    "    }\n",
    "\n",
    "    pipe.set_params(**pipe_params)\n",
    "\n",
    "    start_time = time.time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_pred = [df_median if y<0 else y for y in y_pred] # no negative values\n",
    "\n",
    "    # metrics\n",
    "    score_mae = mean_absolute_error(y_test, y_pred)\n",
    "    score_mse = mean_squared_error(y_test, y_pred)\n",
    "    score_rmse = np.sqrt(score_mse) \n",
    "    score_rmsle = rmsle(y_test, y_pred)\n",
    "    score_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    model_params = str(model[1].get_params())\n",
    "    \n",
    "    param_dict = {\n",
    "        'model': model[1].__class__.__name__,\n",
    "        'model_params': model_params,\n",
    "        'num_trans': num_tr.__class__.__name__,\n",
    "        'cat_trans': cat_tr.__class__.__name__,\n",
    "        'prep_name': main_prep_step_name,\n",
    "        'prep_steps': steps_in_main_prep,\n",
    "        'score_mae': score_mae,\n",
    "        'score_mse': score_mse,\n",
    "        'score_rmse': score_rmse,\n",
    "        'score_rmsle': score_rmsle,\n",
    "        'score_r2': score_r2,\n",
    "        'time_elapsed': end_time - start_time\n",
    "    }\n",
    "\n",
    "    # neptune log\n",
    "    for key, value in param_dict.items():\n",
    "        if key in text_input:\n",
    "            neptune.log_text(key, value)\n",
    "        else:\n",
    "            neptune.log_metric(key, value)\n",
    "    \n",
    "    # add row for local results\n",
    "    models_df = models_df.append(pd.DataFrame(param_dict, index = [0]))\n",
    "\n",
    "models_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_params</th>\n",
       "      <th>num_trans</th>\n",
       "      <th>cat_trans</th>\n",
       "      <th>prep_name</th>\n",
       "      <th>prep_steps</th>\n",
       "      <th>score_mae</th>\n",
       "      <th>score_mse</th>\n",
       "      <th>score_rmse</th>\n",
       "      <th>score_rmsle</th>\n",
       "      <th>score_r2</th>\n",
       "      <th>time_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>25.390761</td>\n",
       "      <td>1791.027460</td>\n",
       "      <td>42.320532</td>\n",
       "      <td>0.412238</td>\n",
       "      <td>0.944849</td>\n",
       "      <td>1.042613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>35.335349</td>\n",
       "      <td>3088.838590</td>\n",
       "      <td>55.577321</td>\n",
       "      <td>0.745846</td>\n",
       "      <td>0.904885</td>\n",
       "      <td>0.693325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>25.412235</td>\n",
       "      <td>1793.003247</td>\n",
       "      <td>42.343869</td>\n",
       "      <td>0.412353</td>\n",
       "      <td>0.944788</td>\n",
       "      <td>0.860783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>35.935276</td>\n",
       "      <td>3117.489269</td>\n",
       "      <td>55.834481</td>\n",
       "      <td>0.769404</td>\n",
       "      <td>0.904003</td>\n",
       "      <td>0.727006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>27.045360</td>\n",
       "      <td>2013.649492</td>\n",
       "      <td>44.873706</td>\n",
       "      <td>0.404867</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.910109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>36.785542</td>\n",
       "      <td>3365.412999</td>\n",
       "      <td>58.012180</td>\n",
       "      <td>0.705086</td>\n",
       "      <td>0.896369</td>\n",
       "      <td>1.274437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>27.286960</td>\n",
       "      <td>1993.316315</td>\n",
       "      <td>44.646571</td>\n",
       "      <td>0.634110</td>\n",
       "      <td>0.938620</td>\n",
       "      <td>0.631282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>34.347730</td>\n",
       "      <td>2836.416376</td>\n",
       "      <td>53.258017</td>\n",
       "      <td>0.857814</td>\n",
       "      <td>0.912658</td>\n",
       "      <td>1.199998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>27.286960</td>\n",
       "      <td>1993.316315</td>\n",
       "      <td>44.646571</td>\n",
       "      <td>0.634110</td>\n",
       "      <td>0.938620</td>\n",
       "      <td>0.610579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>34.347730</td>\n",
       "      <td>2836.416376</td>\n",
       "      <td>53.258017</td>\n",
       "      <td>0.857814</td>\n",
       "      <td>0.912658</td>\n",
       "      <td>1.217865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>29.236406</td>\n",
       "      <td>2186.161051</td>\n",
       "      <td>46.756401</td>\n",
       "      <td>0.659958</td>\n",
       "      <td>0.932681</td>\n",
       "      <td>1.963732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>36.122415</td>\n",
       "      <td>3092.378833</td>\n",
       "      <td>55.609161</td>\n",
       "      <td>0.841339</td>\n",
       "      <td>0.904776</td>\n",
       "      <td>1.738506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>27.951890</td>\n",
       "      <td>2010.772968</td>\n",
       "      <td>44.841643</td>\n",
       "      <td>0.611865</td>\n",
       "      <td>0.938082</td>\n",
       "      <td>0.154269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>37.632445</td>\n",
       "      <td>3221.637365</td>\n",
       "      <td>56.759469</td>\n",
       "      <td>0.816503</td>\n",
       "      <td>0.900796</td>\n",
       "      <td>0.181405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>27.951890</td>\n",
       "      <td>2010.772968</td>\n",
       "      <td>44.841643</td>\n",
       "      <td>0.611865</td>\n",
       "      <td>0.938082</td>\n",
       "      <td>0.172895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>37.632445</td>\n",
       "      <td>3221.637365</td>\n",
       "      <td>56.759469</td>\n",
       "      <td>0.816503</td>\n",
       "      <td>0.900796</td>\n",
       "      <td>0.243432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>28.530257</td>\n",
       "      <td>2114.981938</td>\n",
       "      <td>45.988933</td>\n",
       "      <td>0.585828</td>\n",
       "      <td>0.934873</td>\n",
       "      <td>0.187970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>37.715671</td>\n",
       "      <td>3227.316952</td>\n",
       "      <td>56.809479</td>\n",
       "      <td>0.739567</td>\n",
       "      <td>0.900621</td>\n",
       "      <td>0.206186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>26.180251</td>\n",
       "      <td>1777.897051</td>\n",
       "      <td>42.165117</td>\n",
       "      <td>0.340347</td>\n",
       "      <td>0.945253</td>\n",
       "      <td>2.728065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>34.189149</td>\n",
       "      <td>3024.201427</td>\n",
       "      <td>54.992740</td>\n",
       "      <td>0.487688</td>\n",
       "      <td>0.906876</td>\n",
       "      <td>31.822186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>26.202128</td>\n",
       "      <td>1833.256680</td>\n",
       "      <td>42.816547</td>\n",
       "      <td>0.339270</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>2.558913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>34.252446</td>\n",
       "      <td>3052.362918</td>\n",
       "      <td>55.248194</td>\n",
       "      <td>0.484466</td>\n",
       "      <td>0.906008</td>\n",
       "      <td>33.188503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>27.653512</td>\n",
       "      <td>1985.543447</td>\n",
       "      <td>44.559437</td>\n",
       "      <td>0.346384</td>\n",
       "      <td>0.938859</td>\n",
       "      <td>3.814995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>main_preprocessing</td>\n",
       "      <td>make_dt_columns, change_seasons</td>\n",
       "      <td>36.021292</td>\n",
       "      <td>3411.441984</td>\n",
       "      <td>58.407551</td>\n",
       "      <td>0.468410</td>\n",
       "      <td>0.894951</td>\n",
       "      <td>39.221930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  ... time_elapsed\n",
       "0            XGBRegressor  ...     1.042613\n",
       "1            XGBRegressor  ...     0.693325\n",
       "2            XGBRegressor  ...     0.860783\n",
       "3            XGBRegressor  ...     0.727006\n",
       "4            XGBRegressor  ...     0.910109\n",
       "5            XGBRegressor  ...     1.274437\n",
       "6       CatBoostRegressor  ...     0.631282\n",
       "7       CatBoostRegressor  ...     1.199998\n",
       "8       CatBoostRegressor  ...     0.610579\n",
       "9       CatBoostRegressor  ...     1.217865\n",
       "10      CatBoostRegressor  ...     1.963732\n",
       "11      CatBoostRegressor  ...     1.738506\n",
       "12          LGBMRegressor  ...     0.154269\n",
       "13          LGBMRegressor  ...     0.181405\n",
       "14          LGBMRegressor  ...     0.172895\n",
       "15          LGBMRegressor  ...     0.243432\n",
       "16          LGBMRegressor  ...     0.187970\n",
       "17          LGBMRegressor  ...     0.206186\n",
       "18  RandomForestRegressor  ...     2.728065\n",
       "19  RandomForestRegressor  ...    31.822186\n",
       "20  RandomForestRegressor  ...     2.558913\n",
       "21  RandomForestRegressor  ...    33.188503\n",
       "22  RandomForestRegressor  ...     3.814995\n",
       "23  RandomForestRegressor  ...    39.221930\n",
       "\n",
       "[24 rows x 12 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "423b8b96d4e1ec477b5c8920e05a523372dc7245eb6302a2ca5de114e24c04c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
