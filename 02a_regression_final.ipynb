{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Main info*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Project description:*** https://www.kaggle.com/competitions/bike-sharing-demand/overview/description\n",
    "\n",
    "***Project goal:*** combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n",
    "\n",
    "***Notebook goal:*** make a final set of pipelines to transform the dataset and run them with predifined models in neptune\n",
    "\n",
    "***Suggested evaluation metric:*** Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "***Other used evalutaion metrics*** Mean Absolute Error(MAE), Mean Squared Error(MSE), Root Mean Squared Error(RMSE), R Squared (R2)\n",
    "\n",
    "***Comment*** This notebook uses previous notebooks: <u>01a_regression_pipelines_first_test</u>, <u>01b_regression_pipelines_improved</u> and <u>01c_bonus_meteo_equations_for_atemp</u> in order to build a final  solution for the presented regression problem.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***0. Project preparation*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main upgrades\n",
    "!pip install --upgrade neptune-client\n",
    "!pip install --upgrade neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "import catboost as ctb\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "import eli5\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# minor settings\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "set_config(display='diagram')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# global variables\n",
    "RANDOM_STATE = 0\n",
    "OUTPUT_DIR = 'outputs/'\n",
    "MODELS_DIR = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.5\n",
      "numpy...............1.20.0\n",
      "pandas..............1.2.4\n",
      "sklearn.............0.24.2\n",
      "eli5................0.11.0\n",
      "neptune.............0.15.2\n",
      "catboost............0.25.1\n",
      "lightgbm............3.0.0\n",
      "xgboost.............1.3.2\n"
     ]
    }
   ],
   "source": [
    "# version check\n",
    "def show_version(module_object: object, n: int = 20) -> str:\n",
    "    '''\n",
    "    Check version of different libraries\n",
    "    '''\n",
    "    module_name = getattr(module_object, '__name__')\n",
    "    module_ver = getattr(module_object, '__version__')\n",
    "    dots = '.' * (n - len(module_name))\n",
    "    \n",
    "    print (f'{module_name}{dots}{module_ver}')\n",
    "\n",
    "\n",
    "!python --version\n",
    "module_list = [np, pd, sklearn, eli5, neptune, ctb, lgbm, xgb]\n",
    "for module in module_list:\n",
    "    show_version(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run neptune server\n",
    "def init_neptune(credentials_file = 'neptune_credentials.json'):\n",
    "    '''\n",
    "    Initialize neptune project\n",
    "    '''\n",
    "    with open(credentials_file) as f:\n",
    "        neptune_credentials = json.load(f)\n",
    "\n",
    "    run = neptune.init(\n",
    "        api_token = neptune_credentials['API_TOKEN'],\n",
    "        project_qualified_name = neptune_credentials['PROJECT']\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***1. Load data*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('inputs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {1: ['Clear', 'Few clouds', 'Partly cloudy'],\n",
    "2: ['Mist + Cloudy', 'Mist + Broken clouds', 'Mist + Few clouds', 'Mist'],\n",
    "3: ['Light Snow', 'Light Rain + Thunderstorm + Scattered clouds', 'Light Rain + Scattered clouds'],\n",
    "4: ['Heavy Rain + Ice Pallets + Thunderstorm + Mist', 'Snow + Fog']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2. Custom data classes*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    '''\n",
    "    Change the initial dataset into a new one based\n",
    "    on passed function\n",
    "    '''\n",
    "    # copy parameter introduced to prevent SettingwithCopyWarning\n",
    "    # https://www.dataquest.io/blog/settingwithcopywarning/\n",
    "    def __init__(self, func, copy = True, **kwargs):\n",
    "        self.func = func\n",
    "        self.copy = copy\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        input_df_ = input_df if not self.copy else input_df.copy()\n",
    "        return self.func(input_df_,)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector():\n",
    "    '''\n",
    "    Return a dataframe with predefined columns only\n",
    "    '''\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDroper():\n",
    "    '''\n",
    "    Return a dataframe without selected columns\n",
    "    '''\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3. Custom feature functions*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.1. Add or change functions*** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.1.1. Numerical features functions*** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.1 Main transformation functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(dataset: pd.DataFrame, feats: list, transformer: sklearn.preprocessing._data) -> pd.DataFrame:\n",
    "    '''\n",
    "    Use a transformer to transform selected columns\n",
    "    Examples of transformers: OrdinalEncoder, MinMaxScaler, Normalizer, StandardScaler\n",
    "        Function used in pipeline step.\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        dataframe with transformed columns\n",
    "    '''\n",
    "    dataset[feats] = transformer.fit_transform(dataset[feats])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(dataset: pd.DataFrame, pipeline: Pipeline) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset after pipeline transformations.\n",
    "        Function used in main run step.\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "\n",
    "    Returns: \n",
    "        dataset after pipeline transformation\n",
    "    '''\n",
    "    return pipeline.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.2 Other transformation functions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cols_from_datetime(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Convert a column to datetime format and create new columns: \n",
    "    'year', 'month', 'day', 'hour'\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame \n",
    "    \n",
    "    Returns:\n",
    "        dataset: transformed pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    # convert string to datetime type\n",
    "    dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "    # make new columns from datetime column\n",
    "    dataset['year'] = dataset['datetime'].dt.year\n",
    "    dataset['month'] = dataset['datetime'].dt.month\n",
    "    dataset['day'] = dataset['datetime'].dt.day\n",
    "    dataset['hour'] = dataset['datetime'].dt.hour\n",
    "    dataset['dayofweek'] = dataset['datetime'].dt.dayofweek\n",
    "    dataset['weekend'] = dataset['dayofweek'].map(lambda x: int(x in [6,7]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons_change(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Set proper season duration and change their representation according\n",
    "    to dataset legend\n",
    "\n",
    "    Argument:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    changes = [\n",
    "    ('2011-01-01', '2011-03-19', 4),\n",
    "    ('2011-03-20', '2011-06-20', 1),\n",
    "    ('2011-06-21', '2011-09-22', 2),\n",
    "    ('2011-09-23', '2011-12-20', 3),\n",
    "    ('2011-12-21', '2012-03-19', 4),\n",
    "    ('2012-03-20', '2012-06-19', 1),\n",
    "    ('2012-06-20', '2012-09-21', 2),\n",
    "    ('2012-09-22', '2012-12-20', 3),\n",
    "    ('2012-12-21', '2012-12-31', 4),\n",
    "     ]\n",
    "\n",
    "    for (start_date, end_date, new_season) in changes:\n",
    "        dataset.loc[between_dates(dataset, start_date, '00', end_date, '23').index,'season'] = new_season\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agg_features(dataset: pd.DataFrame, agg_name = np.median) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate a monthly agg_function (like mean, median...) and add it to the dataframe\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: DataFrame with an additional column which contains the agg_function\n",
    "        of bike shares in a month\n",
    "    '''\n",
    "    agg_dataset = dataset[['month', 'year', 'count']].groupby(['month', 'year']).agg(agg_name)\n",
    "    agg_dataset = agg_dataset.reset_index()\n",
    "    agg_dataset = agg_dataset.rename(columns = {'count':str(agg_name.__name__)})\n",
    "    return pd.merge(dataset, agg_dataset, on=['month', 'year'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize_feats(dataset: pd.DataFrame, feats: list = []) -> pd.DataFrame:\n",
    "    '''\n",
    "    Normalize selected features. Equivalent of MinMaxScaler\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    for feature_name in feats:\n",
    "        if feature_name in dataset.columns.tolist():\n",
    "            max_value = dataset[feature_name].max()\n",
    "            min_value = dataset[feature_name].min()\n",
    "            dataset[feature_name] = (dataset[feature_name] - min_value) / (max_value - min_value)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_y_pred(y_pred: np.array) -> np.array:\n",
    "    '''\n",
    "    Correct y_pred values co they don't contain any negative values\n",
    "        Function used in main run step.\n",
    "\n",
    "    Arguments:\n",
    "        y_pred: np.array\n",
    "    \n",
    "    Returns:\n",
    "        y_pred: array wiht no negative values; every negative value\n",
    "        is replaced with median\n",
    "    '''\n",
    "    df_median = np.median(df_train['count']) \n",
    "    y_pred = [df_median if y<0 else y for y in y_pred] \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.1.2. Text (nlp) features functions*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_weather(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Change integers in 'weather' column (1, 2, 3, ...) into a list of weather\n",
    "    phenomena\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        initial dataframe with one extra column (weather_phenomena)\n",
    "    '''\n",
    "    \n",
    "    # split weather phenomena into individual elements\n",
    "    def weather_to_indicidual(x):\n",
    "        x = ','.join([elem for elem in x])\n",
    "        x = re.split(r'[,\\+]', x)\n",
    "\n",
    "        return list(set([elem.strip() for elem in x]))\n",
    "\n",
    "    # change integers in 'weather' column  into list of individual weather phenomena\n",
    "    dataset['weather_phenomena'] = dataset['weather'].map(weather_dict).apply(lambda x: weather_to_indicidual(x))\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconstruct_weather_tokens(dataset: pd.DataFrame, token_col_name: str = 'weather_phenomena') -> pd.DataFrame:\n",
    "    '''\n",
    "    One-hot-encode individual weather phenomena and add them to initial dataframe\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        initial dataframe with an extra columns containing 0 or 1; each column\n",
    "        represents an individual weather phenomena\n",
    "    '''\n",
    "\n",
    "    # make a list of all individual meteorological phenomena \n",
    "    all_phenomena = list(dataset[token_col_name].explode().unique())\n",
    "\n",
    "    def return_cols_with_individual_phenomena(phenomena: pd.Series) -> pd.DataFrame:\n",
    "        '''\n",
    "        Build a dataframe (X) with all_phenomena as columns and put 0 or 1 in \n",
    "        rows representing the presence or absence of a particular phenomena\n",
    "        '''\n",
    "\n",
    "        def phenomenon_in_phenomena(phenomena: pd.Series) -> list:\n",
    "            return [int(phenomenon in phenomena) for phenomenon in all_phenomena]\n",
    "    \n",
    "        X = phenomena.map(phenomenon_in_phenomena).apply(pd.Series)\n",
    "        X.columns = all_phenomena\n",
    "        return X \n",
    "    \n",
    "    X = return_cols_with_individual_phenomena(dataset['weather_phenomena'])\n",
    "\n",
    "    # concatenate columns to original dataset\n",
    "    return pd.concat([dataset, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weather_embeddings(dataset: pd.DataFrame) -> np.array:\n",
    "    '''\n",
    "    Return unique vector representations of the weather column\n",
    "        Function needed for vectorize_weather_nlp function\n",
    "    \n",
    "    Arguments: \n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        array of 4 lists (4 types of weather), each list containing 384 elements\n",
    "    '''\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # randomly chosen pretrained model\n",
    "    model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "    # column of unique meteo phenomena in a 'str' form\n",
    "    uni_meteo_weather = dataset['weather_phenomena'].map(lambda x: ' '.join(x)).unique() \n",
    "\n",
    "    # vector representatnions of 'uni_meteo_weather'\n",
    "    embeddings = model.encode(uni_meteo_weather, convert_to_tensor = False)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_weather_nlp(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return vector representations of the weather column\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset with 384 more columns containing vector representation of sentences\n",
    "        (meteorological phenomena contatenated into a string are treated as sentences)\n",
    "    '''\n",
    "\n",
    "    # unique integers (1, 2, 3, 4) repesenting the weather\n",
    "    uni_num_weather = dataset['weather'].unique()\n",
    "    \n",
    "    # unique embeddings of weather phenomena\n",
    "    embeddings = make_weather_embeddings(dataset)\n",
    "\n",
    "    # dictionary for pandas mapping\n",
    "    embeddings_dict = dict(zip(uni_num_weather, embeddings))\n",
    "\n",
    "    # one additional column containing a 384-element list in each cell\n",
    "    dataset['weather_embeddings'] = dataset['weather'].map(embeddings_dict)\n",
    "\n",
    "    # split 'weather_embeddings' into 384 individual columns and concat them to the original dataset\n",
    "    dataset = pd.concat([dataset, pd.DataFrame(dataset['weather_embeddings'].tolist())], axis = 1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cos_sim_weather(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate cosinus similarity between 4 weather types and concat\n",
    "    4 additional columns to the original dataset containing calculated\n",
    "    similarity\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        original dataset with 4 extra columns representing cosine similarity \n",
    "        between weather types\n",
    "    '''\n",
    "\n",
    "    # construct 4x4 array of cosine similarity\n",
    "    cos_sims_array = peek_weather_cos_sim(dataset, return_dataframe= False)\n",
    "\n",
    "    # dictionary with cos_sims for pandas mapping \n",
    "    # {1: [1.000000 0.897642 0.728108 0.566511], 2: [...], 3: [...], 4: [...]}\n",
    "    cos_sims_dict = dict(zip(list(range(1,5)), cos_sims_array))\n",
    "\n",
    "    # map dictionary to pandas DataFrame as a new column\n",
    "    dataset['cos_sims'] = dataset['weather'].map(cos_sims_dict)\n",
    "\n",
    "    # deconstruct mapped column to 4 individual columns\n",
    "    cos_sims_new_columns = pd.DataFrame(dataset['cos_sims'].tolist(), columns = [f'cos_sim_weather_{n}' for n in list(range(1, 5))])\n",
    "\n",
    "    # delete not needed column\n",
    "    del dataset['cos_sims']\n",
    "\n",
    "    return pd.concat([dataset, cos_sims_new_columns], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.2. Select functions*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between_dates(dataset: pd.DataFrame, start_date: str, start_time: str, end_date: str, end_time: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Show dataframe between two dates and timestamps\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame \n",
    "        start_date: date which the dataset must be trimmed from\n",
    "        start_time: hour of day from start_date\n",
    "        end_date: date which the dataset must be trimmed to\n",
    "        end_time: hour of day from end_date\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame between (start_date, start_time) and (end_date, end_time)\n",
    "    '''\n",
    "\n",
    "    start_dt = f'{start_date} {start_time}:00:00'\n",
    "    end_dt = f'{end_date} {end_time}:00:00'\n",
    "    mask = (dataset['datetime'] >= start_dt) & (dataset['datetime'] <= end_dt)\n",
    "    return dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_obvious(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset withouth some obviously unneeded columns\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    black_list = ['Unnamed: 0', 'datetime', 'casual', 'registered']\n",
    "    \n",
    "    feats = [feat for feat in dataset.columns.tolist() if feat not in black_list]\n",
    "\n",
    "    return dataset[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dtypes(dataset: pd.DataFrame, dtypes = np.number) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataset with a specific datatype\n",
    "\n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dataset: pandas DataFrame\n",
    "    '''\n",
    "    return dataset.select_dtypes(include=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y(dataset: pd.DataFrame, test_size: int = 0.3):\n",
    "    '''\n",
    "    Split dataframe info train and valid set\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: pandas DataFrame\n",
    "        test_size: test size split\n",
    "    \n",
    "    Returns:\n",
    "        two DataFrames and two DataSeries containing independent and target variables\n",
    "    '''\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(dataset.drop('count', axis = 1), dataset['count'], test_size = test_size, random_state = RANDOM_STATE)\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_results_files(models_df: pd.DataFrame, topn: int = 5, metric: str = 'score_rmsle', lower_is_better: bool = True) -> list :\n",
    "    '''\n",
    "    Return a tuple containing file names with best n models and datasets\n",
    "\n",
    "    Arguments:\n",
    "        models_df: dataframe of models after main run loop\n",
    "        topn: how many top models are being returned\n",
    "        metric: evaluation metric which is being taken into account when choosing top models\n",
    "        lower_is_better: specify is a lower value of metric means a better model performence\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples; each tuple consist of a csv file name and a file model name\n",
    "        example: [(dataset1.csv, xgboost.model), (dataset1.csv, catboost.model), (dataset2.csv, xgboost.model)]\n",
    "    '''\n",
    "    if lower_is_better:\n",
    "        best_models = models_df.sort_values(by='score_rmsle', ascending= True).head(topn)\n",
    "    else:\n",
    "        best_models =models_df.sort_values(by='score_rmsle', ascending= False).head(topn)\n",
    "    \n",
    "    return [(f'{pipeline_name}.csv', f'{model_name}-{pipeline_name}.model') for pipeline_name, model_name in zip(best_models['pipeline_name'], best_models['model_custom_name'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_results_df(models_df: pd.DataFrame, topn: int = 5, metric: str = 'score_rmsle', lower_is_better: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a dataframe with best n experiments\n",
    "\n",
    "    Arguments:\n",
    "        models_df: dataframe of models after main run loop\n",
    "        topn: how many top models are being returned\n",
    "        metric: evaluation metric which is being taken into account when choosing top models\n",
    "        lower_is_better: specify is a lower value of metric means a better model performence\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe trimmed to only  topn experiments base on a dataframe generated\n",
    "        with the run_experiments function\n",
    "    '''\n",
    "    if lower_is_better:\n",
    "        best_models = models_df.sort_values(by='score_rmsle', ascending= True).head(topn)\n",
    "    else:\n",
    "        best_models =models_df.sort_values(by='score_rmsle', ascending= False).head(topn)\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***3.3. Additional functions*** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_weather_cos_sim(dataset: pd.DataFrame, return_dataframe = True):\n",
    "    '''\n",
    "    Show cosine similarity between different types of weather for a transformed dataset\n",
    "        Function needed for add_cos_sim_weather function\n",
    "    \n",
    "    Arguments: \n",
    "        dataset: pandas DataFrame (transformed dataset containing 'weather_phenomena' column)\n",
    "    \n",
    "    Returns:\n",
    "       4 x 4 DataFrame with cosine similarity of weather types \n",
    "       or\n",
    "       dataframe with cosine similarity of weather types\n",
    "        \n",
    "    '''\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "    # unique embeddings of weather phenomena    \n",
    "    embeddings = make_weather_embeddings(dataset)\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    cos_sims = cosine_similarity(embeddings)\n",
    "\n",
    "    # how many different weather types there is\n",
    "    no_weather_types = len(weather_dict)\n",
    "\n",
    "    if return_dataframe:\n",
    "        return  pd.DataFrame(cos_sims, columns = list(range(1,no_weather_types + 1)), index = list(range(1,no_weather_types + 1)))\n",
    "    else:\n",
    "        return cos_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single(dataset: pd.DataFrame, pipeline: Pipeline, model, feats: list, metric, show_feature_importance: bool = True) -> tuple:\n",
    "    '''\n",
    "    Make a single prediction for a single case. This is needed when we want to quickly:\n",
    "    1) transform 2) split into train and valid 3) fit 4) predict 5) score\n",
    "    a dataset and a model\n",
    "\n",
    "    If a pipeline is specified the data will be transformed according to this pipeline, otherwise\n",
    "    an already prepared dataset must be provided as input.\n",
    "\n",
    "    If feats are provided, the dataset is restricted to only those feats. We can skip specifying\n",
    "    the feets if a pipeline already transformed the data properly to just the feats we want.\n",
    "\n",
    "    Arguments:\n",
    "        dataset: raw or transformed dataset\n",
    "        pipeline: pipeline to transform the dataset (optional)\n",
    "        model: specified model for fitting the dataset\n",
    "        feats: features to be included in a dataset (optional)\n",
    "        metric: metric for scoring\n",
    "        show_feature_importance: show or hide eli5 feature importande\n",
    "    Returns:\n",
    "        transformed dataset, model predict score\n",
    "    '''\n",
    "    def show_weights(model, return_weights_dataframe = True):\n",
    "        if return_weights_dataframe:\n",
    "            return pd.read_html(eli5.show_weights(model, feature_names = X_train.columns.tolist()).data)[0]\n",
    "        else:\n",
    "                return pd.read_html(eli5.show_weights(model, feature_names = X_train.columns.tolist()).data)[0]['Feature'].tolist()\n",
    "\n",
    "\n",
    "    if pipeline is not None:\n",
    "        dataset = transform_dataset(dataset, pipeline)\n",
    "\n",
    "    if feats is not None:\n",
    "        X_train, X_valid, y_train, y_valid = make_X_y(dataset[feats])\n",
    "    else:\n",
    "        X_train, X_valid, y_train, y_valid = make_X_y(dataset)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_valid) \n",
    "    y_pred = correct_y_pred(y_pred)\n",
    "\n",
    "    score = metric(y_valid, y_pred)\n",
    "    if show_feature_importance:\n",
    "        show_weights(model)\n",
    "    return dataset, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_results_files(filename: str, models_df):\n",
    "    '''\n",
    "    Save datasets names and models names to a *.npy file\n",
    "\n",
    "    Arguments:\n",
    "        filename: specified file name \n",
    "        models_df: dataset with summary of all models performence\n",
    "    '''\n",
    "    np.save(filename, np.array(top_results_files(models_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***4. Evaluation metrics*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> np.float64:\n",
    "    '''\n",
    "    The Root Mean Squared Log Error (RMSLE) metric \n",
    "\n",
    "    Arguments: \n",
    "        y_true: the ground truth labels given in the dataset\n",
    "        y_pred: our predictions\n",
    "        \n",
    "    Returns: \n",
    "        The RMSLE score\n",
    "    '''\n",
    "\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5. Pipelines and models*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_pipeline = Pipeline(steps = [\n",
    "    ('drop obvious', DataTransformer(drop_obvious))\n",
    "], verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocess = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('min_max_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['atemp'], MinMaxScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season']))),\n",
    "    ('tokenize_weather',\n",
    "                        DataTransformer(tokenize_weather)),\n",
    "    ('deconstruct_tokens',\n",
    "                        DataTransformer(deconstruct_weather_tokens)),\n",
    "    ('nlp',\n",
    "                        DataTransformer(vectorize_weather_nlp)),\n",
    "    ('add_cos_sim',\n",
    "                        DataTransformer(add_cos_sim_weather)),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['weather_phenomena', 'weather_embeddings', 'humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nlp_pipeline = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('min_max_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['atemp'], MinMaxScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season']))),\n",
    "    ('tokenize_weather',\n",
    "                        DataTransformer(tokenize_weather)),\n",
    "    ('deconstruct_tokens',\n",
    "                        DataTransformer(deconstruct_weather_tokens)),\n",
    "    ('add_cos_sim',\n",
    "                        DataTransformer(add_cos_sim_weather)),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['weather_phenomena', 'humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nlp_pipeline_no_cos_sim = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('min_max_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['atemp'], MinMaxScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season']))),\n",
    "    ('tokenize_weather',\n",
    "                        DataTransformer(tokenize_weather)),\n",
    "    ('deconstruct_tokens',\n",
    "                        DataTransformer(deconstruct_weather_tokens)),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['weather_phenomena', 'humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_dummies_different_scaler = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('standard_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['atemp'], StandardScaler()))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season', 'weather']))),\n",
    "    ('drop_columns', \n",
    "                        ColumnDroper(['humidity', 'windspeed', 'temp'])),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_drop_humidity_windspeed_temp = Pipeline(steps = [\n",
    "    ('make_dt_columns', \n",
    "                        DataTransformer(make_cols_from_datetime)),\n",
    "    ('change_seasons', \n",
    "                        DataTransformer(seasons_change)),\n",
    "    ('add_means', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.mean))),\n",
    "    ('add_medians', \n",
    "                        DataTransformer(lambda df: generate_agg_features(df, np.median))),\n",
    "    ('standard_scale', \n",
    "                        DataTransformer(lambda df: transform_columns(df, ['atemp', 'temp', 'windspeed', 'humidity'], StandardScaler()))),\n",
    "    ('dummies', \n",
    "                        DataTransformer(lambda df: pd.get_dummies(df, columns = ['season', 'weather']))),\n",
    "    ('drop_obvious', \n",
    "                        DataTransformer(drop_obvious)),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "xparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': RANDOM_STATE, 'verbosity':0, 'use_label_encoder': False }\n",
    "xparams_more_n = {'n_estimators': 200, 'max_depth': 10, 'random_state': RANDOM_STATE, 'verbosity':0, 'use_label_encoder': False }\n",
    "cparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': RANDOM_STATE , 'silent': True}\n",
    "lparams = {'n_estimators': 100, 'max_depth': 10, 'random_state': RANDOM_STATE, 'verbosity':-1, 'silent': True}\n",
    "lparams_more_depth = {'n_estimators': 100, 'max_depth': 20, 'random_state': RANDOM_STATE, 'verbosity':-1, 'silent': True}\n",
    "rfparams = {'random_state': RANDOM_STATE}\n",
    "dparams = {'strategy': 'median'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    ('basic_pipeline', basic_pipeline, 'No transfomations (other then essential drop columns'),\n",
    "    ('main_preprocess', main_preprocess, 'Default preprocess pipeline with nlp and with \"humidity\", \"casual\", \"windspeed\" columns dropped'),\n",
    "    ('no_nlp', no_nlp_pipeline, 'Same as default pipeline but with no nlp functions with \"humidity\", \"casual\", \"windspeed\" columns dropped'),\n",
    "    ('no_nlp_no_cos_sim', no_nlp_pipeline_no_cos_sim, 'Same as deafult pipeline but with no nlp and no cos_sim with \"humidity\", \"casual\", \"windspeed\" columns dropped'),\n",
    "    ('more_dummies_different_scaler_no_nlp', more_dummies_different_scaler, 'No nlp, no cos sim, dummies include: season and weather with \"humidity\", \"casual\", \"windspeed\" columns dropped'),\n",
    "    ('more_dummies_different_scaler_no_nlp_keep_3variables', no_drop_humidity_windspeed_temp, 'No nlp, no cos sim, dummies include: season and weather; \"humidity\", \"casual\", \"windspeed\" not dropped but scaled'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('XGBoostRegressor_100_estim', xgb.XGBRegressor(**xparams)),\n",
    "    ('XGBoostRegressor_200_estim', xgb.XGBRegressor(**xparams_more_n)),\n",
    "    ('CatBoostRregressor',  ctb.CatBoostRegressor(**cparams)),\n",
    "    ('LGBMRegressor_max_depth_10', lgbm.LGBMRegressor(**lparams)),\n",
    "    ('LGBMRegressor_max_depth_20', lgbm.LGBMRegressor(**lparams_more_depth)),\n",
    "    ('RandomForest', RandomForestRegressor()),\n",
    "    ('DummyRegressor', DummyRegressor(**dparams)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***6. Main run*** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IMPORTANT`\n",
    "\n",
    "Use `dump_pickled_models_to_neptune` with caution. It can send hundreds of MB of pickled models to neptune server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(pipelines, models = models, experiments_common_name = 'test-experiments',  use_neptune = True, pickle_models = False, dump_pickled_models_to_neptune = False):\n",
    "    \n",
    "    # empty dataframe for locally keeping track of results\n",
    "    models_df = pd.DataFrame()\n",
    "\n",
    "    # total number of all experiments\n",
    "    no_experiments = len(list(product(pipelines, models)))\n",
    "\n",
    "    print(f'Running {no_experiments} experiments')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # initialize neptune if needed\n",
    "    if use_neptune:\n",
    "        run = init_neptune()\n",
    "    \n",
    "\n",
    "    e = 1 # first experiment number\n",
    "    for p, pipeline_obj in enumerate(pipelines, 1):\n",
    "        pipeline_name = pipeline_obj[0] # name of pipeline\n",
    "        pipeline = pipeline_obj[1] # pipeline instance\n",
    "        pipeline_comment = pipeline_obj[2] # pipeline description\n",
    "\n",
    "        print(f'Transforming dataset using |{pipeline_name}|... (transformation {p}/{len(pipelines)})')\n",
    "        print(f'{\"=\"*60}')\n",
    "\n",
    "        # Data transformation\n",
    "        df = transform_dataset(df_train, pipeline) # transform dataset using a pipline\n",
    "        X_train, X_valid, y_train, y_valid = make_X_y(df) # split dataset into train and valid\n",
    "        \n",
    "        # Model application on transformed data \n",
    "        for m, model_object in enumerate(models, 1):\n",
    "\n",
    "            model_name = model_object[0] # custom name of a  model (like 'XGBoostRegressor')\n",
    "            model = model_object[1] # model instance\n",
    "\n",
    "            print('\\n')\n",
    "            print(f'Fitting... (model {m}/{len(models)})')\n",
    "            print(model_name)\n",
    "        \n",
    "            if use_neptune:\n",
    "                neptune.create_experiment(f'{experiments_common_name}-{e}') # name of experiment\n",
    "            \n",
    "            e += 1\n",
    "            start_time = time.time() # time fitting\n",
    "            model.fit(X_train, y_train) # fit the model\n",
    "            end_time = time.time()\n",
    "\n",
    "            # file pickling\n",
    "            if pickle_models: \n",
    "                \n",
    "                # file names\n",
    "                data_file_name = f'{pipeline_name}.csv'\n",
    "                model_file_name = f'{model_name}-{pipeline_name}.model'\n",
    "\n",
    "                if 'Unnamed: 0' in df.columns: del df['Unnamed: 0']\n",
    "                df.to_csv(f'{OUTPUT_DIR}{data_file_name}') # save dataframe locally\n",
    "                \n",
    "                # save model locally\n",
    "                with open(f'{MODELS_DIR}{model_file_name}', 'wb') as f:\n",
    "                    pickle.dump(model, f) # pickle a model\n",
    "                \n",
    "                if use_neptune and dump_pickled_models_to_neptune:\n",
    "                    # if specified dump pickled files to neptune\n",
    "                    neptune.log_artifact(model_file_name)\n",
    "                    neptune.log_artifact(data_file_name)\n",
    "\n",
    "            y_pred = model.predict(X_valid) # predicted values\n",
    "            y_pred = correct_y_pred(y_pred)\n",
    "\n",
    "            # metrics (scores)\n",
    "            score_mae = mean_absolute_error(y_valid, y_pred)\n",
    "            score_mse = mean_squared_error(y_valid, y_pred)\n",
    "            score_rmse = np.sqrt(score_mse) \n",
    "            score_rmsle = rmsle(y_valid, y_pred)\n",
    "            score_r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "            model_params = str(model.get_params()) # model parameters\n",
    "            \n",
    "            # dictionary of all variables that are supposed to be logged\n",
    "            param_dict = {\n",
    "                'pipeline_name': pipeline_name,\n",
    "                'pipeline_steps': str(list(pipeline.named_steps.keys())),\n",
    "                'pipeline_comment': pipeline_comment,\n",
    "                'feats': str(X_train.columns.tolist()),\n",
    "                'n_feats': len(X_train.columns.tolist()), ### added\n",
    "                'fs_method': 'no_selection',\n",
    "                'fs_param': '',\n",
    "                'fs_param_value': 0,\n",
    "                'model': model.__class__.__name__,\n",
    "                'model_custom_name': model_name,\n",
    "                'model_params': model_params,\n",
    "                'score_mae': score_mae,\n",
    "                'score_mse': score_mse,\n",
    "                'score_rmse': score_rmse,\n",
    "                'score_rmsle': score_rmsle,\n",
    "                'score_r2': score_r2,\n",
    "                'time_elapsed': end_time - start_time\n",
    "            }\n",
    "\n",
    "            # log into neptune if needed\n",
    "            if use_neptune:\n",
    "                score_metrics = [elem for elem in list(param_dict.keys()) if elem.startswith('score_')] + ['time_elapsed', 'n_feats', 'fs_param_value']\n",
    "                \n",
    "                # log values depending on their type (str or float)\n",
    "                for key, value in param_dict.items():\n",
    "                    if key not in score_metrics:\n",
    "                        neptune.log_text(key, value)\n",
    "                    else:\n",
    "                        neptune.log_metric(key, value)\n",
    "                \n",
    "            # add row into summary dataframe for local results\n",
    "            models_df = models_df.append(pd.DataFrame(param_dict, index = [0]))\n",
    "    \n",
    "    models_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # end neptune instance\n",
    "    if use_neptune:\n",
    "        neptune.stop()\n",
    "    \n",
    "    return models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = run_experiments(pipelines, models, use_neptune= False, pickle_models= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>pipeline_steps</th>\n",
       "      <th>pipeline_comment</th>\n",
       "      <th>feats</th>\n",
       "      <th>n_feats</th>\n",
       "      <th>fs_method</th>\n",
       "      <th>fs_param</th>\n",
       "      <th>fs_param_value</th>\n",
       "      <th>model</th>\n",
       "      <th>model_custom_name</th>\n",
       "      <th>model_params</th>\n",
       "      <th>score_mae</th>\n",
       "      <th>score_mse</th>\n",
       "      <th>score_rmse</th>\n",
       "      <th>score_rmsle</th>\n",
       "      <th>score_r2</th>\n",
       "      <th>time_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_100_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>117.568922</td>\n",
       "      <td>27413.487615</td>\n",
       "      <td>165.570189</td>\n",
       "      <td>1.300570</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>2.077066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_200_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>120.200671</td>\n",
       "      <td>28794.651284</td>\n",
       "      <td>169.689868</td>\n",
       "      <td>1.310784</td>\n",
       "      <td>0.113325</td>\n",
       "      <td>2.688413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>CatBoostRregressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>107.201296</td>\n",
       "      <td>21331.481809</td>\n",
       "      <td>146.053010</td>\n",
       "      <td>1.276259</td>\n",
       "      <td>0.343139</td>\n",
       "      <td>1.250663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_10</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>107.972832</td>\n",
       "      <td>21391.046882</td>\n",
       "      <td>146.256784</td>\n",
       "      <td>1.288039</td>\n",
       "      <td>0.341305</td>\n",
       "      <td>0.556958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_20</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>107.718162</td>\n",
       "      <td>21386.646607</td>\n",
       "      <td>146.241740</td>\n",
       "      <td>1.287986</td>\n",
       "      <td>0.341440</td>\n",
       "      <td>0.220453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>109.580280</td>\n",
       "      <td>23374.623360</td>\n",
       "      <td>152.887617</td>\n",
       "      <td>1.243114</td>\n",
       "      <td>0.280224</td>\n",
       "      <td>2.713230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>basic_pipeline</td>\n",
       "      <td>['drop obvious']</td>\n",
       "      <td>No transfomations (other then essential drop c...</td>\n",
       "      <td>['season', 'holiday', 'workingday', 'weather',...</td>\n",
       "      <td>8</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_100_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>24.869353</td>\n",
       "      <td>1857.934551</td>\n",
       "      <td>43.103765</td>\n",
       "      <td>0.396175</td>\n",
       "      <td>0.942789</td>\n",
       "      <td>10.840070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_200_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.013816</td>\n",
       "      <td>1871.048207</td>\n",
       "      <td>43.255615</td>\n",
       "      <td>0.404771</td>\n",
       "      <td>0.942385</td>\n",
       "      <td>26.722949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>CatBoostRregressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>27.160699</td>\n",
       "      <td>1995.790765</td>\n",
       "      <td>44.674274</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.938544</td>\n",
       "      <td>4.404709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_10</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.922237</td>\n",
       "      <td>2102.292492</td>\n",
       "      <td>45.850763</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>1.630742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_20</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>26.976258</td>\n",
       "      <td>1976.165881</td>\n",
       "      <td>44.454087</td>\n",
       "      <td>0.546402</td>\n",
       "      <td>0.939148</td>\n",
       "      <td>6.951989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>26.178396</td>\n",
       "      <td>1982.232337</td>\n",
       "      <td>44.522268</td>\n",
       "      <td>0.334133</td>\n",
       "      <td>0.938961</td>\n",
       "      <td>84.175290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>main_preprocess</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Default preprocess pipeline with nlp and with ...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>418</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.002008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_100_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.209120</td>\n",
       "      <td>1873.586728</td>\n",
       "      <td>43.284948</td>\n",
       "      <td>0.408043</td>\n",
       "      <td>0.942307</td>\n",
       "      <td>1.487029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_200_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.358280</td>\n",
       "      <td>1889.073703</td>\n",
       "      <td>43.463476</td>\n",
       "      <td>0.408781</td>\n",
       "      <td>0.941830</td>\n",
       "      <td>2.825461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>CatBoostRregressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>26.691917</td>\n",
       "      <td>1911.869824</td>\n",
       "      <td>43.724934</td>\n",
       "      <td>0.623860</td>\n",
       "      <td>0.941128</td>\n",
       "      <td>2.750885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_10</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.922237</td>\n",
       "      <td>2102.292492</td>\n",
       "      <td>45.850763</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.320349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_20</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>26.976258</td>\n",
       "      <td>1976.165881</td>\n",
       "      <td>44.454087</td>\n",
       "      <td>0.546402</td>\n",
       "      <td>0.939148</td>\n",
       "      <td>0.376984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>25.745126</td>\n",
       "      <td>1898.102144</td>\n",
       "      <td>43.567214</td>\n",
       "      <td>0.334597</td>\n",
       "      <td>0.941552</td>\n",
       "      <td>7.845413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as default pipeline but with no nlp funct...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_100_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.209120</td>\n",
       "      <td>1873.586728</td>\n",
       "      <td>43.284948</td>\n",
       "      <td>0.408043</td>\n",
       "      <td>0.942307</td>\n",
       "      <td>1.732943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_200_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.358280</td>\n",
       "      <td>1889.073703</td>\n",
       "      <td>43.463476</td>\n",
       "      <td>0.408781</td>\n",
       "      <td>0.941830</td>\n",
       "      <td>4.477798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>CatBoostRregressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>26.657420</td>\n",
       "      <td>1931.863545</td>\n",
       "      <td>43.952970</td>\n",
       "      <td>0.646309</td>\n",
       "      <td>0.940512</td>\n",
       "      <td>2.838057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_10</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.922237</td>\n",
       "      <td>2102.292492</td>\n",
       "      <td>45.850763</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.182930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_20</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>26.976258</td>\n",
       "      <td>1976.165881</td>\n",
       "      <td>44.454087</td>\n",
       "      <td>0.546402</td>\n",
       "      <td>0.939148</td>\n",
       "      <td>0.127851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>25.952551</td>\n",
       "      <td>1902.143504</td>\n",
       "      <td>43.613570</td>\n",
       "      <td>0.333347</td>\n",
       "      <td>0.941427</td>\n",
       "      <td>4.019652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>no_nlp_no_cos_sim</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>Same as deafult pipeline but with no nlp and n...</td>\n",
       "      <td>['holiday', 'workingday', 'weather', 'atemp', ...</td>\n",
       "      <td>30</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_100_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.063424</td>\n",
       "      <td>1837.846831</td>\n",
       "      <td>42.870116</td>\n",
       "      <td>0.401472</td>\n",
       "      <td>0.943407</td>\n",
       "      <td>1.030805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_200_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.204164</td>\n",
       "      <td>1851.054639</td>\n",
       "      <td>43.023885</td>\n",
       "      <td>0.404708</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>6.579423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>CatBoostRregressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>27.048130</td>\n",
       "      <td>1952.208913</td>\n",
       "      <td>44.183808</td>\n",
       "      <td>0.629591</td>\n",
       "      <td>0.939886</td>\n",
       "      <td>0.868397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_10</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.573382</td>\n",
       "      <td>2064.036380</td>\n",
       "      <td>45.431667</td>\n",
       "      <td>0.567303</td>\n",
       "      <td>0.936442</td>\n",
       "      <td>0.241929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_20</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.811896</td>\n",
       "      <td>2079.501190</td>\n",
       "      <td>45.601548</td>\n",
       "      <td>0.584423</td>\n",
       "      <td>0.935966</td>\n",
       "      <td>0.215767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>26.024832</td>\n",
       "      <td>1915.727176</td>\n",
       "      <td>43.769021</td>\n",
       "      <td>0.336090</td>\n",
       "      <td>0.941009</td>\n",
       "      <td>3.465718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'atemp', 'year', 'mo...</td>\n",
       "      <td>19</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_100_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.214282</td>\n",
       "      <td>1705.434050</td>\n",
       "      <td>41.296901</td>\n",
       "      <td>0.435547</td>\n",
       "      <td>0.947485</td>\n",
       "      <td>1.246286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>XGBoostRegressor_200_estim</td>\n",
       "      <td>{'objective': 'reg:squarederror', 'base_score'...</td>\n",
       "      <td>25.226140</td>\n",
       "      <td>1704.790804</td>\n",
       "      <td>41.289112</td>\n",
       "      <td>0.437203</td>\n",
       "      <td>0.947504</td>\n",
       "      <td>2.197621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>CatBoostRregressor</td>\n",
       "      <td>{'loss_function': 'RMSE', 'silent': True, 'max...</td>\n",
       "      <td>27.562787</td>\n",
       "      <td>1964.135064</td>\n",
       "      <td>44.318563</td>\n",
       "      <td>0.658735</td>\n",
       "      <td>0.939518</td>\n",
       "      <td>0.938049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_10</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.272607</td>\n",
       "      <td>1919.793440</td>\n",
       "      <td>43.815448</td>\n",
       "      <td>0.579045</td>\n",
       "      <td>0.940884</td>\n",
       "      <td>0.159997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>LGBMRegressor_max_depth_20</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': None...</td>\n",
       "      <td>27.110330</td>\n",
       "      <td>1886.024792</td>\n",
       "      <td>43.428387</td>\n",
       "      <td>0.573337</td>\n",
       "      <td>0.941924</td>\n",
       "      <td>0.128647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
       "      <td>25.418610</td>\n",
       "      <td>1698.082229</td>\n",
       "      <td>41.207793</td>\n",
       "      <td>0.335817</td>\n",
       "      <td>0.947711</td>\n",
       "      <td>4.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>more_dummies_different_scaler_no_nlp_keep_3var...</td>\n",
       "      <td>['make_dt_columns', 'change_seasons', 'add_mea...</td>\n",
       "      <td>No nlp, no cos sim, dummies include: season an...</td>\n",
       "      <td>['holiday', 'workingday', 'temp', 'atemp', 'hu...</td>\n",
       "      <td>22</td>\n",
       "      <td>no_selection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>{'constant': None, 'quantile': None, 'strategy...</td>\n",
       "      <td>138.770361</td>\n",
       "      <td>35329.691978</td>\n",
       "      <td>187.961943</td>\n",
       "      <td>1.462466</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.001477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        pipeline_name  \\\n",
       "0                                      basic_pipeline   \n",
       "1                                      basic_pipeline   \n",
       "2                                      basic_pipeline   \n",
       "3                                      basic_pipeline   \n",
       "4                                      basic_pipeline   \n",
       "5                                      basic_pipeline   \n",
       "6                                      basic_pipeline   \n",
       "7                                     main_preprocess   \n",
       "8                                     main_preprocess   \n",
       "9                                     main_preprocess   \n",
       "10                                    main_preprocess   \n",
       "11                                    main_preprocess   \n",
       "12                                    main_preprocess   \n",
       "13                                    main_preprocess   \n",
       "14                                             no_nlp   \n",
       "15                                             no_nlp   \n",
       "16                                             no_nlp   \n",
       "17                                             no_nlp   \n",
       "18                                             no_nlp   \n",
       "19                                             no_nlp   \n",
       "20                                             no_nlp   \n",
       "21                                  no_nlp_no_cos_sim   \n",
       "22                                  no_nlp_no_cos_sim   \n",
       "23                                  no_nlp_no_cos_sim   \n",
       "24                                  no_nlp_no_cos_sim   \n",
       "25                                  no_nlp_no_cos_sim   \n",
       "26                                  no_nlp_no_cos_sim   \n",
       "27                                  no_nlp_no_cos_sim   \n",
       "28               more_dummies_different_scaler_no_nlp   \n",
       "29               more_dummies_different_scaler_no_nlp   \n",
       "30               more_dummies_different_scaler_no_nlp   \n",
       "31               more_dummies_different_scaler_no_nlp   \n",
       "32               more_dummies_different_scaler_no_nlp   \n",
       "33               more_dummies_different_scaler_no_nlp   \n",
       "34               more_dummies_different_scaler_no_nlp   \n",
       "35  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "36  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "37  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "38  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "39  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "40  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "41  more_dummies_different_scaler_no_nlp_keep_3var...   \n",
       "\n",
       "                                       pipeline_steps  \\\n",
       "0                                    ['drop obvious']   \n",
       "1                                    ['drop obvious']   \n",
       "2                                    ['drop obvious']   \n",
       "3                                    ['drop obvious']   \n",
       "4                                    ['drop obvious']   \n",
       "5                                    ['drop obvious']   \n",
       "6                                    ['drop obvious']   \n",
       "7   ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "8   ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "9   ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "10  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "11  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "12  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "13  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "14  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "15  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "16  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "17  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "18  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "19  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "20  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "21  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "22  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "23  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "24  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "25  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "26  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "27  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "28  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "29  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "30  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "31  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "32  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "33  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "34  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "35  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "36  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "37  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "38  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "39  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "40  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "41  ['make_dt_columns', 'change_seasons', 'add_mea...   \n",
       "\n",
       "                                     pipeline_comment  \\\n",
       "0   No transfomations (other then essential drop c...   \n",
       "1   No transfomations (other then essential drop c...   \n",
       "2   No transfomations (other then essential drop c...   \n",
       "3   No transfomations (other then essential drop c...   \n",
       "4   No transfomations (other then essential drop c...   \n",
       "5   No transfomations (other then essential drop c...   \n",
       "6   No transfomations (other then essential drop c...   \n",
       "7   Default preprocess pipeline with nlp and with ...   \n",
       "8   Default preprocess pipeline with nlp and with ...   \n",
       "9   Default preprocess pipeline with nlp and with ...   \n",
       "10  Default preprocess pipeline with nlp and with ...   \n",
       "11  Default preprocess pipeline with nlp and with ...   \n",
       "12  Default preprocess pipeline with nlp and with ...   \n",
       "13  Default preprocess pipeline with nlp and with ...   \n",
       "14  Same as default pipeline but with no nlp funct...   \n",
       "15  Same as default pipeline but with no nlp funct...   \n",
       "16  Same as default pipeline but with no nlp funct...   \n",
       "17  Same as default pipeline but with no nlp funct...   \n",
       "18  Same as default pipeline but with no nlp funct...   \n",
       "19  Same as default pipeline but with no nlp funct...   \n",
       "20  Same as default pipeline but with no nlp funct...   \n",
       "21  Same as deafult pipeline but with no nlp and n...   \n",
       "22  Same as deafult pipeline but with no nlp and n...   \n",
       "23  Same as deafult pipeline but with no nlp and n...   \n",
       "24  Same as deafult pipeline but with no nlp and n...   \n",
       "25  Same as deafult pipeline but with no nlp and n...   \n",
       "26  Same as deafult pipeline but with no nlp and n...   \n",
       "27  Same as deafult pipeline but with no nlp and n...   \n",
       "28  No nlp, no cos sim, dummies include: season an...   \n",
       "29  No nlp, no cos sim, dummies include: season an...   \n",
       "30  No nlp, no cos sim, dummies include: season an...   \n",
       "31  No nlp, no cos sim, dummies include: season an...   \n",
       "32  No nlp, no cos sim, dummies include: season an...   \n",
       "33  No nlp, no cos sim, dummies include: season an...   \n",
       "34  No nlp, no cos sim, dummies include: season an...   \n",
       "35  No nlp, no cos sim, dummies include: season an...   \n",
       "36  No nlp, no cos sim, dummies include: season an...   \n",
       "37  No nlp, no cos sim, dummies include: season an...   \n",
       "38  No nlp, no cos sim, dummies include: season an...   \n",
       "39  No nlp, no cos sim, dummies include: season an...   \n",
       "40  No nlp, no cos sim, dummies include: season an...   \n",
       "41  No nlp, no cos sim, dummies include: season an...   \n",
       "\n",
       "                                                feats  n_feats     fs_method  \\\n",
       "0   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "1   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "2   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "3   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "4   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "5   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "6   ['season', 'holiday', 'workingday', 'weather',...        8  no_selection   \n",
       "7   ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "8   ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "9   ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "10  ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "11  ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "12  ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "13  ['holiday', 'workingday', 'weather', 'atemp', ...      418  no_selection   \n",
       "14  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "15  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "16  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "17  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "18  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "19  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "20  ['holiday', 'workingday', 'weather', 'atemp', ...       34  no_selection   \n",
       "21  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "22  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "23  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "24  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "25  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "26  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "27  ['holiday', 'workingday', 'weather', 'atemp', ...       30  no_selection   \n",
       "28  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "29  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "30  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "31  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "32  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "33  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "34  ['holiday', 'workingday', 'atemp', 'year', 'mo...       19  no_selection   \n",
       "35  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "36  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "37  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "38  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "39  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "40  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "41  ['holiday', 'workingday', 'temp', 'atemp', 'hu...       22  no_selection   \n",
       "\n",
       "   fs_param fs_param_value                  model           model_custom_name  \\\n",
       "0                                    XGBRegressor  XGBoostRegressor_100_estim   \n",
       "1                                    XGBRegressor  XGBoostRegressor_200_estim   \n",
       "2                               CatBoostRegressor          CatBoostRregressor   \n",
       "3                                   LGBMRegressor  LGBMRegressor_max_depth_10   \n",
       "4                                   LGBMRegressor  LGBMRegressor_max_depth_20   \n",
       "5                           RandomForestRegressor                RandomForest   \n",
       "6                                  DummyRegressor              DummyRegressor   \n",
       "7                                    XGBRegressor  XGBoostRegressor_100_estim   \n",
       "8                                    XGBRegressor  XGBoostRegressor_200_estim   \n",
       "9                               CatBoostRegressor          CatBoostRregressor   \n",
       "10                                  LGBMRegressor  LGBMRegressor_max_depth_10   \n",
       "11                                  LGBMRegressor  LGBMRegressor_max_depth_20   \n",
       "12                          RandomForestRegressor                RandomForest   \n",
       "13                                 DummyRegressor              DummyRegressor   \n",
       "14                                   XGBRegressor  XGBoostRegressor_100_estim   \n",
       "15                                   XGBRegressor  XGBoostRegressor_200_estim   \n",
       "16                              CatBoostRegressor          CatBoostRregressor   \n",
       "17                                  LGBMRegressor  LGBMRegressor_max_depth_10   \n",
       "18                                  LGBMRegressor  LGBMRegressor_max_depth_20   \n",
       "19                          RandomForestRegressor                RandomForest   \n",
       "20                                 DummyRegressor              DummyRegressor   \n",
       "21                                   XGBRegressor  XGBoostRegressor_100_estim   \n",
       "22                                   XGBRegressor  XGBoostRegressor_200_estim   \n",
       "23                              CatBoostRegressor          CatBoostRregressor   \n",
       "24                                  LGBMRegressor  LGBMRegressor_max_depth_10   \n",
       "25                                  LGBMRegressor  LGBMRegressor_max_depth_20   \n",
       "26                          RandomForestRegressor                RandomForest   \n",
       "27                                 DummyRegressor              DummyRegressor   \n",
       "28                                   XGBRegressor  XGBoostRegressor_100_estim   \n",
       "29                                   XGBRegressor  XGBoostRegressor_200_estim   \n",
       "30                              CatBoostRegressor          CatBoostRregressor   \n",
       "31                                  LGBMRegressor  LGBMRegressor_max_depth_10   \n",
       "32                                  LGBMRegressor  LGBMRegressor_max_depth_20   \n",
       "33                          RandomForestRegressor                RandomForest   \n",
       "34                                 DummyRegressor              DummyRegressor   \n",
       "35                                   XGBRegressor  XGBoostRegressor_100_estim   \n",
       "36                                   XGBRegressor  XGBoostRegressor_200_estim   \n",
       "37                              CatBoostRegressor          CatBoostRregressor   \n",
       "38                                  LGBMRegressor  LGBMRegressor_max_depth_10   \n",
       "39                                  LGBMRegressor  LGBMRegressor_max_depth_20   \n",
       "40                          RandomForestRegressor                RandomForest   \n",
       "41                                 DummyRegressor              DummyRegressor   \n",
       "\n",
       "                                         model_params   score_mae  \\\n",
       "0   {'objective': 'reg:squarederror', 'base_score'...  117.568922   \n",
       "1   {'objective': 'reg:squarederror', 'base_score'...  120.200671   \n",
       "2   {'loss_function': 'RMSE', 'silent': True, 'max...  107.201296   \n",
       "3   {'boosting_type': 'gbdt', 'class_weight': None...  107.972832   \n",
       "4   {'boosting_type': 'gbdt', 'class_weight': None...  107.718162   \n",
       "5   {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...  109.580280   \n",
       "6   {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "7   {'objective': 'reg:squarederror', 'base_score'...   24.869353   \n",
       "8   {'objective': 'reg:squarederror', 'base_score'...   25.013816   \n",
       "9   {'loss_function': 'RMSE', 'silent': True, 'max...   27.160699   \n",
       "10  {'boosting_type': 'gbdt', 'class_weight': None...   27.922237   \n",
       "11  {'boosting_type': 'gbdt', 'class_weight': None...   26.976258   \n",
       "12  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   26.178396   \n",
       "13  {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "14  {'objective': 'reg:squarederror', 'base_score'...   25.209120   \n",
       "15  {'objective': 'reg:squarederror', 'base_score'...   25.358280   \n",
       "16  {'loss_function': 'RMSE', 'silent': True, 'max...   26.691917   \n",
       "17  {'boosting_type': 'gbdt', 'class_weight': None...   27.922237   \n",
       "18  {'boosting_type': 'gbdt', 'class_weight': None...   26.976258   \n",
       "19  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   25.745126   \n",
       "20  {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "21  {'objective': 'reg:squarederror', 'base_score'...   25.209120   \n",
       "22  {'objective': 'reg:squarederror', 'base_score'...   25.358280   \n",
       "23  {'loss_function': 'RMSE', 'silent': True, 'max...   26.657420   \n",
       "24  {'boosting_type': 'gbdt', 'class_weight': None...   27.922237   \n",
       "25  {'boosting_type': 'gbdt', 'class_weight': None...   26.976258   \n",
       "26  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   25.952551   \n",
       "27  {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "28  {'objective': 'reg:squarederror', 'base_score'...   25.063424   \n",
       "29  {'objective': 'reg:squarederror', 'base_score'...   25.204164   \n",
       "30  {'loss_function': 'RMSE', 'silent': True, 'max...   27.048130   \n",
       "31  {'boosting_type': 'gbdt', 'class_weight': None...   27.573382   \n",
       "32  {'boosting_type': 'gbdt', 'class_weight': None...   27.811896   \n",
       "33  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   26.024832   \n",
       "34  {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "35  {'objective': 'reg:squarederror', 'base_score'...   25.214282   \n",
       "36  {'objective': 'reg:squarederror', 'base_score'...   25.226140   \n",
       "37  {'loss_function': 'RMSE', 'silent': True, 'max...   27.562787   \n",
       "38  {'boosting_type': 'gbdt', 'class_weight': None...   27.272607   \n",
       "39  {'boosting_type': 'gbdt', 'class_weight': None...   27.110330   \n",
       "40  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...   25.418610   \n",
       "41  {'constant': None, 'quantile': None, 'strategy...  138.770361   \n",
       "\n",
       "       score_mse  score_rmse  score_rmsle  score_r2  time_elapsed  \n",
       "0   27413.487615  165.570189     1.300570  0.155855      2.077066  \n",
       "1   28794.651284  169.689868     1.310784  0.113325      2.688413  \n",
       "2   21331.481809  146.053010     1.276259  0.343139      1.250663  \n",
       "3   21391.046882  146.256784     1.288039  0.341305      0.556958  \n",
       "4   21386.646607  146.241740     1.287986  0.341440      0.220453  \n",
       "5   23374.623360  152.887617     1.243114  0.280224      2.713230  \n",
       "6   35329.691978  187.961943     1.462466 -0.087909      0.000000  \n",
       "7    1857.934551   43.103765     0.396175  0.942789     10.840070  \n",
       "8    1871.048207   43.255615     0.404771  0.942385     26.722949  \n",
       "9    1995.790765   44.674274     0.648352  0.938544      4.404709  \n",
       "10   2102.292492   45.850763     0.586519  0.935264      1.630742  \n",
       "11   1976.165881   44.454087     0.546402  0.939148      6.951989  \n",
       "12   1982.232337   44.522268     0.334133  0.938961     84.175290  \n",
       "13  35329.691978  187.961943     1.462466 -0.087909      0.002008  \n",
       "14   1873.586728   43.284948     0.408043  0.942307      1.487029  \n",
       "15   1889.073703   43.463476     0.408781  0.941830      2.825461  \n",
       "16   1911.869824   43.724934     0.623860  0.941128      2.750885  \n",
       "17   2102.292492   45.850763     0.586519  0.935264      0.320349  \n",
       "18   1976.165881   44.454087     0.546402  0.939148      0.376984  \n",
       "19   1898.102144   43.567214     0.334597  0.941552      7.845413  \n",
       "20  35329.691978  187.961943     1.462466 -0.087909      0.001144  \n",
       "21   1873.586728   43.284948     0.408043  0.942307      1.732943  \n",
       "22   1889.073703   43.463476     0.408781  0.941830      4.477798  \n",
       "23   1931.863545   43.952970     0.646309  0.940512      2.838057  \n",
       "24   2102.292492   45.850763     0.586519  0.935264      0.182930  \n",
       "25   1976.165881   44.454087     0.546402  0.939148      0.127851  \n",
       "26   1902.143504   43.613570     0.333347  0.941427      4.019652  \n",
       "27  35329.691978  187.961943     1.462466 -0.087909      0.000000  \n",
       "28   1837.846831   42.870116     0.401472  0.943407      1.030805  \n",
       "29   1851.054639   43.023885     0.404708  0.943000      6.579423  \n",
       "30   1952.208913   44.183808     0.629591  0.939886      0.868397  \n",
       "31   2064.036380   45.431667     0.567303  0.936442      0.241929  \n",
       "32   2079.501190   45.601548     0.584423  0.935966      0.215767  \n",
       "33   1915.727176   43.769021     0.336090  0.941009      3.465718  \n",
       "34  35329.691978  187.961943     1.462466 -0.087909      0.001306  \n",
       "35   1705.434050   41.296901     0.435547  0.947485      1.246286  \n",
       "36   1704.790804   41.289112     0.437203  0.947504      2.197621  \n",
       "37   1964.135064   44.318563     0.658735  0.939518      0.938049  \n",
       "38   1919.793440   43.815448     0.579045  0.940884      0.159997  \n",
       "39   1886.024792   43.428387     0.573337  0.941924      0.128647  \n",
       "40   1698.082229   41.207793     0.335817  0.947711      4.371600  \n",
       "41  35329.691978  187.961943     1.462466 -0.087909      0.001477  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "423b8b96d4e1ec477b5c8920e05a523372dc7245eb6302a2ca5de114e24c04c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
